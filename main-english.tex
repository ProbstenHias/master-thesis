% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-
% !TEX root = ./main-english.tex

% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-side printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={Implementing Variational Quantum Algorithms as Compositions of Reusable Microservice-based Plugins},
  author={Matthias Weilinger},
  type=master,
  institute=iaas, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Prof.\ Dr.\ Dr.\ h.\ c.\ Frank Leymann},
  supervisor={M.Sc.\ Philipp Wundrack,\\M.Sc.\ Fabian Bühler},
  startdate={April 19, 2023},
  enddate={October 19, 2023}
]{scientific-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create picures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\ifdeutsch
  \section*{Kurzfassung}
\else
  \section*{Abstract}
\fi

<Short summary of the thesis>

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
\ifdeutsch
  \listof{Listing}{Verzeichnis der Listings}
\else
  \listof{Listing}{List of Listings}
\fi

%mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
%Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
\ifdeutsch
  \listof{Algorithmus}{Verzeichnis der Algorithmen}
\else
  \listof{Algorithmus}{List of Algorithms}
\fi
%\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig

% Abkürzungsverzeichnis
\printnoidxglossaries

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Notes what I have done so far}
\begin{itemize}
  \item Added recursive parsing of the plugin folders so that subfolders are also parsed
  \item created a callable plugin that gets the data parsed from its invoker via the database
  \item created a invoker that calls the callable plugin
  \item user can now select a plugin from the list of callable plugins, the list is narrowed down to the plugins that are compatible via the tag field
  \item creating a method to get the plugin name from the plugin URL
  \item made the callee plugin to multistep, to demonstrate that any amount of steps can be done in invoked plugin
  \item started with an optimizer plugin that gives the first frontend for the user to select the objective-function-plugin
  \item create an objective-function-plugin that takes means squared error as an objective function
  \item todo: creating a method to get the plugin metadata from the plugin URL (this is needed in order to get the entry points of the plugin)
  \item identified three key problems:
  \begin{itemize}
    \item Reliable way to pass a callback function to the callee plugin
    \item A way to get a list of interaction endpoints of the callee plugin
    \item A way to get the plugin metadata from the plugin URL
  \end{itemize}
  \item 19.05: I am currently working on a big problem
  \begin{itemize}
    \item I have the optimizer plugin which should call the objective-function-plugin so that it can ask the user for the hyperparameters of the objective function
    \item For that reason I need to add a next step as a celery task.
    \item The thing is i don't want to handle the next task like other multistep plugins where they share a db id since the objective-function-plugin should be able to stand on its own.
    \item Therefore, I pass a callback function to the objective-function-plugin which it should call when it is done with the setup.
    \item works all fine like that
    \item The problem right now is how i call the objective-function-plugin from the optimizer plugin
    \item I need to add it as a step, which is usually done via the add\_step celery task
    \item This task though needs a db\_id which I don't want to add.
    \item When adding none it works to call the objective-function-plugin and its also possbile to call the callback function
    \item but the problem is that with the add\_step task the celery task is not called asyncronously
    \item therefore when the callback function is called the optimizer plugin is not yet finished with the add\_step task
    \item usually one would go in the called multistep and do a clear\_previous\_steps call to make sure that the previous steps are finished
    \item but this call does not work since the objective-function-plugin does not have a db\_id
    \item thinking right now....
    \item maybe we can finish the task in the callback function of the optimizer plugin?
    \item Still 19.05 here we are again
    \item I have now a working solution for the problem
    \item the proposed solution of clearing the previous steps in the callback function did work
    \item the problem was that i forgot to add the call db\_task.save(cammit=True) in the callback function
    \item this is needed to save the changes to the database without it the cleared variable is not saved
  \end{itemize}
  \item 20.05: Today we give the objective-function-plugin the ability to calculate the loss function
  \begin{itemize}
    \item On callback we give the optimizer plugin the url of the calculateLoss function url
    \item The optimizer plugin then calls the calculateLoss function
  \end{itemize}
  \item 22.05: we continue with the upper
  \begin{itemize}
    \item it is now possible to calculate the loss function
    \item we simply call the calculation enpoint with a post request
    \item we pass all the necessary data in the body of the request
    \item we have a special schema for that
    \item current problem: passing the data to the minimize function of scipy.optimize
    \item we have the hyperparameters as a dict to keep it as generic as possible
    \item but the minimize function needs the hyperparameters as a list
    \item and also the loss function needs the hyperparameters as a dict, so lets see how we can solve this
    \item we maybe did it by just passing the hyperparameters as a dict to both of the functions
    \item now we have the problem that the content type of the input file is not correctly set by postman
    \item we just change the code to accept the content type but lets not forget to change it back
  \end{itemize}
  \item 23.05: today we do some cleanup
  \begin{itemize}
    \item make the import relative so that it works with the docker container
    \item remove all no ops tasks
  \end{itemize}
  \item 02.06:
  \begin{itemize}
    \item we want to skip the optimizer UI since we do not need any more input data
    \item tried to set the cleared value of the created step to true, but it did not work, since only the process step is started is by clicking submit in the ui
    \item trying to chain the tasks directly in the callback function, which works
  \end{itemize}
  \item 05.06:
  \begin{itemize}
    \item now I want to call the objective-function-plugin via the entry points that I get through the metadata, this is needed to make the plugins more generic
    \item I have a problem where the shared schemas can not be imported since the NumPy package cannot be found, added the NumPy package to the requirements for the opt plugin, but this did not solve the problems
    \item maybe this will be solved by creating a coordinator plugin that lives in the top level of the plugin folder
  \end{itemize}
  \item 12.06:
  \begin{itemize}
    \item Creating a top level coordinator plugin was not the solution. If an init file is present no further plugins will be loaded from the folder
    \item the shared schemas are now part of the coordinator plugin
    \item Created a new infrastructure for the plugins with a coordinator plugin (diagram will be needed)
    \item today we solve the following problem:
    \item coordinator waits for the optimizer plugin to finish and writes the result to a file
    \item for this the coordinator polls the task api to check if the minimizer task is finished
    \item when it is it writes the result to a file
    \item I should move the callback URL away from the query parameters to the body of the request, maybe to the form
    \item next I should read about how neural networks work and how to minimize them
  \end{itemize}
  \item 13.06:
  \begin{itemize}
    \item i have made a decision on how to handle the callback url from the ui to the processing endpoint
    \item until now the callback url was passed as a query parameter
    \item now i want to pass it into the form that is rendered by the UI
    \item it should be a hidden field
    \item i have to make and change so that i can pass multiple schemas to the render function and set which fields should be hidden
  \end{itemize}
  \item 16.06:
  \begin{itemize}
    \item when an invoked plugin now makes a callback to its invoker it only passes back the endpoint for the calculation endpoint.
    \item it does not pass any hyperparameters back since it should own the hyperparameters and not the invoker
    \item i now want to have a look of how loss functions are called in python and how hyperparameters are passed to them
    \item with this information I want to create interaction endpoints for the objective-function-plugin that are completely generic, i.e. the hyperparameters are passed to the endpoint as a dict
    \item as an example of how a generic method could look like i will have a look at the scipy.optimize.minimize function
    \item I now interaction endpoints as and additional list to entry points
  \end{itemize}
  \item 25.06:
  \begin{itemize}
    \item i have added a functionality where I use the blinker library to create a signal that is emitted when the status of a task changes
    \item this is used by the coordinator plugin to check if the minimization task is finished
    \item the coordinator plugin passes the callback URL to the minimization calculation endpoint
    \item this endpoint registers the URL to the db
    \item the signal handlers makes a post request to this URL when the status of the task changes
    \item I have moved all shared schemas into a separate folder
    \item I have moved all utilities concerning plugin interactions into a separate folder
  \end{itemize}
  \item 26.06:
  \begin{itemize}
    \item I developed a way to pass any amount of parameters to the processing endpoint of the invoked plugin UI
    \item I added an interaction endpoint that is used to invoke the invoked plugin
    \item this interaction endpoint allows any number of parameters
    \item the endpoint saves these parameters to the database
    \item it then adds the entry points of the invoked plugin as a step
    \item it passes the processing URL with the db id as a query parameter to the invoked plugin
    \item the invoked plugin then calls the processing endpoint with the db id as a query parameter
    \item here it starts a new db task with the arguments that were passed to the invoked plugin
  \end{itemize}
\end{itemize}


\chapter{Define the plugins}
\begin{itemize}
  \item \textbf{ObjectiveFunction}: This plugin should have the following steps
  \begin{itemize}
    \item \textbf{ /get hyperparameterUI }: This step should let the user select the hyperparameters of the objective function
    \item \textbf{ /post ObjectiveFunctionSetup}: This step should setup the objective function with set a database id for future reference of the parameter.
    Then it should store the following information to the database:
    \begin{itemize}
      \item hyperparameters
      \item more stuff?? %%% TODO specify what more stuff
    \end{itemize}
    Then it should call the optimizer callback function to indicate that the setup is done. Pass the url of the calculateLoss function as a parameter.
    \item \textbf{ /post CalculateLoss (dbID) }: this step should trigger the calculation of the loss function and should return it.
  \end{itemize}
  \item \textbf{Optimizer}: This plugin should have the following steps:
    \begin{itemize}
      \item \textbf{ /get setup UI }: Let the user select the objective-function-plugin, dataset, target variable, and the optimization algorithm
      \item \textbf{ /post setup }: This step should setup the optimizer with set a database id for future reference of the parameter.
      Then it should call the objective function entry point to setup the objective function. Pass the url of the optimizer callback function as a parameter.
      \item \textbf{ /post callback }: This endpoint should be called by the objective-function-plugin to indicate that the setup is done.
      It should then start the optimization process.
      \item \textbf{ /post optimize }: This step should trigger the optimization process.
      It should loop the optimization function until a stop condition is met.
      In each iteration it should call the objective-function-plugin to calculate the loss function.
    \end{itemize}
\end{itemize}

\chapter{Introduction}

This thesis starts with \cref{chap:k2}.

We can also typeset \verb|<text>verbatim text</text>|.
Backticks are also rendered correctly: \verb|`words in backticks`|.

\chapter{Background}
\label{chap:literatureReview}
\section{Optimization Algorithms}
\label{sec:optimizationAlgorithms}
\subsection{Objective Functions}
\label{subsec:objectiveFunctions}
\glspl{of}, serving as the cornerstone of optimization problems, form the foundation for a wide range of computational algorithms and models.
They provide a metric to gauge the performance of a given model, solution, or set of parameters.
\glspl{of} are the heart of many optimization problems.
The goal is to minimize or maximize these functions depending on the context and requirements \cite{Weinan2017}.
In the context of \glspl{vqa}, the primary objective is to minimize the function.

The core inputs to a \gls{of} typically encompass data points (denoted as $x$), corresponding labels or outcomes (represented by $y$), and a set of parameters or weights (often symbolized by $\theta$ or $w$).
These parameters dictate how the model responds to the input data and makes its predictions.
Additionally, certain \glspl{of} may also include hyperparameters as input, which control the behavior and complexity of the model.
In the context of optimization problems, the role of an \gls{of} is to capture both the problem we're attempting to solve and the strategy by which we're trying to solve it.
It provides a measure of the 'goodness' or 'fitness' of our current solution or parameters, and the aim is to adjust these parameters to improve this measure.

One example of an \gls{of} is the Lasso (Least Absolute Shrinkage and Selection Operator) Loss function.
The Lasso loss function is represented as:
\[
L(Y, X, W, \lambda) = ||Y - XW||^2_2 + \lambda ||W||_1
\]
In this equation:
- \(Y\) is the vector of observed values.
- \(X\) is the matrix of input data points.
- \(W\) is the vector of weights, the parameters of the model.
- \(\lambda\) is the regularization parameter, a non-negative hyperparameter.

This function consists of two terms:
\begin{enumerate}
  \item The first term \(||Y - XW||^2_2\) is the mean squared error between the predicted and actual outcomes.
  It measures the discrepancy between the model's predictions and the true values.
  \item The second term \(\lambda ||W||_1\) is a regularization term, where \(||W||_1\) represents the L1 norm (sum of absolute values) of the weight vector.
  This term penalizes the absolute size of the coefficients, encouraging them to be small.
\end{enumerate}
The hyperparameter \(\lambda\) governs the trade-off between these two terms.
When \(\lambda = 0\), the \gls{of} reduces to ordinary least squares regression, and the weights are chosen to minimize the mean squared error alone.
As \(\lambda\) increases, more weight is given to the regularization term, and the solution becomes more sparse (i.e., more of the weights are driven to zero).
This can help to prevent overfitting by effectively reducing the complexity of the model \cite{ShalevShwartz2014}

\subsection{Minimization Functions}
\label{subsec:minimizationFunctions}
Minimization functions, often referred to as optimization algorithms, play a pivotal role in a vast array of computational models and algorithms.
In essence, they serve to iteratively enhance the parameters of a model to reduce the value of the \gls{of}.
The goal of these minimization functions is to find the optimal set of parameters that yield the lowest possible value of the \gls{of} within the constraints of the problem \cite{Nocedal2006}.

The process of optimization involves a search through the parameter space.
This search can be visualized as navigating a landscape of hills and valleys, with each point in the landscape corresponding to a different set of parameters, and the height at each point representing the value of the \gls{of}.
The goal of the minimization function is to find the lowest point in this landscape, corresponding to the minimum value of the \gls{of} \cite{Goodfellow2017}.

The core inputs to a minimization function are the initial parameters of the model or weights (denoted as \(\theta\) or \(w\)),
the \gls{of} that needs to be minimized, and the gradients of the \gls{of} with respect to the parameters.
Additionally, certain minimization functions may also include hyperparameters as input, which control the behavior and complexity of the optimization process \cite{Virtanen2020}.
For instance, the learning rate is a typical hyperparameter that determines the step size in each iteration of the optimization process.

There are numerous minimization functions used in computational problems, each with its own strengths and weaknesses.
These range from simple methods such as gradient descent, to more complex ones such as the Newton's method, stochastic gradient descent (SGD), RMSprop, and Adam.

One of the most fundamental and widely used minimization functions is the Gradient Descent.
To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.

The update rule of gradient descent is given as:

\[
\theta_{t+1} = \theta_t - \alpha \nabla F(\theta_t)
\]

In this formula:

\begin{itemize}
  \item \(\theta_{t+1}\) represents the parameters at the next time step.
  \item \(\theta_t\) represents the current parameters.
  \item \(\alpha\) is the learning rate, a positive scalar determining the size of the step.
  \item \(\nabla F(\theta_t)\) is the gradient of the of.
\end{itemize}

Here, the \gls{of} \(F\) is assumed to be a differentiable function.
The gradient \(\nabla F(\theta_t)\) provides the direction of the steepest ascent at the point \(\theta_t\), and \(-\nabla F(\theta_t)\) provides the direction of steepest descent.
By taking a step in this direction, we move towards the minimum of the function.

The size of the steps taken is determined by the learning rate \(\alpha\), which is a hyperparameter that must be set before the learning process begins.
The learning rate controls how fast or slow we move towards the optimal weights.
If the learning rate is very large, we may skip the optimal solution.
If it is too small, we may need too many iterations to converge to the best values.

The choice of minimization function can significantly influence the efficiency and success of the optimization process.
While some minimization functions may perform well on certain problems, they may not yield similar results on others.
Therefore, understanding the underlying mechanisms of these functions and their suitability to the specific problem at hand is crucial.

\section{Quantum Computing}
\label{sec:quantumComputing}
Quantum computing is a cutting-edge field that exploits the principles of quantum mechanics to process information.
Unlike classical computers that use bits (0s and 1s) to store and process information, quantum computers use quantum bits, or "qubits."
Qubits, through the phenomena of superposition and entanglement, can exist in multiple states at once and be correlated with each other in ways that classical bits cannot \cite{Nielsen2010}.

Superposition allows a qubit to be in a state that is a combination of both 0 and 1, with a certain probability for each.
This property enables quantum computers to perform many calculations simultaneously, vastly increasing their potential computational power.
Entanglement, on the other hand, allows qubits that are entangled to be intimately linked regardless of the distance separating them.
A change in the state of one will instantaneously affect the state of the other, a phenomenon that Einstein famously called "spooky action at a distance" \cite{Einstein1935}.
This property is essential for many quantum algorithms, quantum error correction, and quantum teleportation, making it a fundamental resource in quantum information processing \cite{Nielsen2010,Preskill1998}.


\section{Variational Quantum Algorithms}
\label{sec:variationalQuantumAlgorithms}

\glspl{vqa} bring together the principles of quantum computing and optimization in a unique and powerful way.
They are a class of hybrid quantum-classical algorithms that leverage the strengths of both quantum and classical computing to solve complex problems \cite{McClean2016}.

The main concept of \glspl{vqa} is to use a sequence of quantum operations (a "quantum circuit") controlled by certain parameters.
These parameters are adjusted using classical optimization techniques with the aim of solving a specific problem.
This problem, in many cases, involves finding the lowest energy state, or "ground state", of a quantum system, a problem that maps to finding the minimum of a particular function \cite{Peruzzo2013}.

By leveraging classical optimization algorithms, \glspl{vqa} become more resistant to quantum errors, as the majority of the computation is performed on a classical computer.
This combination of quantum and classical resources makes \glspl{vqa} a promising type of algorithm for near-term quantum devices \cite{Moll2017}.

\section{QHana}
\label{sec:qhana}

QHana, short for Quantum Humanities Analysis tool, is a unique application developed in the domain of Digital Humanities (DH).
It provides a platform for users to experiment with various machine learning algorithms on specified datasets.
With the advent of quantum computers and their availability in the cloud, QHana has become a viable tool for evaluating the potential advantages of quantum algorithms in the DH community.

QHana is designed to be extensible, allowing the integration of new data sources and quantum algorithms as plugins.
However, plugins are specifically built for specific applications, limiting their reusability in other applications.
Moreover, plugins for an application have to be developed in the same programming language as the application.
Even if a plugin can be reused in another application, its UI has to adapt to the new application, otherwise users may fail to understand the plugin's functionality.
To address this limitation, QHana is built on a novel concept of Reusable Microservice-based Plugins (RAMPs).
This allows microservices with an integrated UI to be used as plugins by multiple applications, enhancing the reusability of the plugins across different applications \cite{Buehler2022}.

In the context of \glspl{vqa}, this thesis aims to extend the capabilities of QHana by developing a novel approach where plugins can interact with each other.
By leveraging the principles of quantum computing and optimization, \glspl{vqa} can be implemented as interchangeable plugins within the QHana platform.
This means that different components of a \gls{vqa}, such as the minimizer and the \gls{of}, can be developed as separate plugins.
These plugins can then interact, communicate, and collaborate to achieve the desired optimization results.
This modular approach not only enhances the flexibility and scalability of the system but also promotes reusability, as individual plugins can be utilized in various combinations to tackle different optimization problems.


\section{RESTful API Design}
In the evolving landscape of software design, microservices have emerged as a preferred architectural style, prized for their modularity, scalability, and independent deployability.
At the heart of QHAna's design is a microservices-based plugin approach, which facilitates its dynamic and extensible nature.
Central to the orchestration of these microservices is the application of RESTful API design.
Representational State Transfer (REST) is an architectural style that sets forth constraints for creating web services.
RESTful APIs, built upon these constraints, are pivotal in ensuring seamless communication between individual microservices, thereby enabling efficient data exchange and service integration.

In platforms like QHAna, where the next step is on plugin interactions, the role of RESTful APIs becomes even more pronounced.
Given that each plugin in QHAna is essentially a microservice, the ability for these plugins to interact, share data, and even leverage functionalities from one another is crucial.
A well-designed RESTful interface ensures that these plugins can dynamically discover and communicate with each other, making the system adaptable and extensible.
This not only promotes interoperability but also enhances the overall resilience, robustness and security of the platform.

For those seeking a deeper dive into the intricacies of RESTful design in microservices architectures, influential works by Fielding \cite{Fielding2000} and practical insights by Richardson and Amundsen \cite{Richardson2013} are recommended.


\chapter{Related Work}

\cite{Beisel2023} \cite{Beisel2023a} \cite{Thullier2021}

\chapter{Methodology}
\label{chap:methodology}
\section{Concept Development for Plugin Interaction}
\label{sec:conceptDevelopment}

\chapter{Implementation}
\label{chap:implementation}
\section{Directory Structure and Plugin Loading}
\label{sec:directoryStructure}

QHana maintains two primary directories for plugin management: the \textit{plugins} and the \textit{stable plugins} directories.
The former hosts plugins undergoing development, while the latter contains plugins that are deployment-ready.
Within these directories, individual plugins are neatly organized into dedicated subfolders.
Upon initiation, QHana scans and loads plugins from these designated subfolders.

The focus of this thesis, the optimization plugin, resides in the \textit{plugins/optimization} directory.
To promote clarity and a structured layout, further divisions were made:

\begin{itemize}
  \item \textit{plugins/optimization/coordinator} – For the main optimization plugin.
  \item \textit{plugins/optimization/objective\_functions} – Where each \gls{of} plugin occupies its respective subfolder.
  \item \textit{plugins/optimization/minimizer} – Where each minimizer plugin occupies its respective subfolder.
\end{itemize}

Given that QHana's native architecture doesn't support the direct loading of plugins from nested subdirectories, a recursive plugin loader was developed for this purpose.
This loader traverses through the \textit{plugins} directory and its subdirectories.
The presence of an \textit{\_\_init\_\_.py} file within a folder acts as a confirmation for the plugin's legitimacy.
If any plugin needs to be excluded from the loading process, a \textit{.ignore} file is placed in its respective folder to act as a loading deterrent.

To ensure that the recursive process doesn’t compromise system performance, the recursion depth is capped at 4.
This threshold sufficiently accommodates the current plugin structure but can be adjusted upwards if future needs arise.

The folder hierarchy is further enriched by the inclusion of an \textit{interaction\_utils} directory, dedicated to housing utility functions for generalized plugin interactions.
Additionally, there's a \textit{shared} directory, which stores data structures and schemas utilized across the plugins related to optimization plugin.
A visual representation of the final folder structure, with all plugins implemented for this thesis, is shown in \cref{fig:folderStructure}.


\begin{figure}[h!]
  \dirtree{%
      .1 plugins.
      .2 optimizer.
      .3 coordinator.
      .3 interaction\_utils.
      .3 minimizer.
      .4 scipy\_minimizer.
      .4 scipy\_minimizer\_grad.
      .3 objective\_functions.
      .4 hinge\_loss.
      .4 neural\_network.
      .4 ridge\_loss.
      .3 shared.
  }
  \caption{QHana Plugin Folder Structure.}
  \label{fig:folderStructure}
\end{figure}


\section{Plugin Infrastructure}
\label{sec:pluginInfrastructure}
\section{Plugin Interaction}
\label{sec:pluginInteraction}
\section{Minimizer Plugin}
\label{sec:minimizerPlugin}
\section{Objective Function Plugins}
\label{sec:objectiveFunctionPlugins}


\chapter{Evaluation}
\label{chap:evaluation}
\section{Sample Machine Learning Experiment}
\label{sec:sampleMachineLearningExperiment}
\section{Interchangeability of Plugins}
\label{sec:interchangeabilityOfPlugins}
\section{Performance Analysis}
\label{sec:performanceAnalysis}

The performance of the system is evaluated by measuring the time the system takes to complete a machine learning experiment.
The time that it takes for a user to input the data is not measured.
This benchmark should only measure the time that it takes for the system to complete the experiment.
We compare the performance of the plugin based system with the performance of a jupyter notebook based system.
There are several steps that are benchmarked in the plugin based system, since there is user interaction between the steps.
\begin{itemize}
  \item The time that it takes after the user has selected the plugins until the user can input the hyperparameters for the of plugin.
  \item The time that it takes after the user has input the of plugin hyperparameters until the user can input the hyperparameters for the minimizer plugin.
  \item The time that it takes after the user has input the minimizer plugin hyperparameters until the user gets the result of the experiment.
\end{itemize}
The times of those steps are summed up to get the total time that it takes for the plugin based system to complete the experiment.
The time that it takes for the jupyter notebook based system is measured from the start of the notebook until the user gets the result of the experiment.
The time is measured by adding a timestamp at the start of the step and at the end of the step.
The difference between the two timestamps is the time that it takes for the step to complete.
The time that it takes for the plugin based system to complete the experiment is compared to the time that it takes for the jupyter notebook based system to complete the experiment.
In order to get a more accurate result, the experiment is repeated several times and the average time is taken.
To make it as comparable as possible, the same datasets and hyperparameters are used for both systems.
The output file of the experiment should also have the same exact format.

Since the benchmark of the plugin based system shows worse performance than the jupyter notebook based system, we want to explore where the most time is spent.
Therefore, we measure the time it takes to call to calculate the loss function.
In case of the plugin based system this includes the network latency between the plugins.
We want to see how much time is spent in the network and how much time is spent in the actual calculation of the loss function.

For more sophisticated loss functions (in this case the neural network) we want to see how much time is spent for the setup of the neural network.
Since a microservice does not have state, the neural network has to be setup for every call of to calculate loss function.

\section{Hardware and System Specifications}

The benchmarks were conducted on a MacBook Pro. The detailed specifications of the machine are as follows:

\begin{itemize}
    \item \textbf{Model:} MacBookPro18,1
    \item \textbf{Processor (CPU):} Apple M1 Pro
    \item \textbf{Memory (RAM):} 32 GB (hw.memsize: 34359738368 bytes)
    \item \textbf{Graphics Processing Unit (GPU):}
    \begin{itemize}
        \item Chipset Model: Apple M1 Pro
        \item Type: GPU
        \item Bus: Built-In
        \item Total Number of Cores: 16
        \item Metal Support: Metal 3
    \end{itemize}
    \item \textbf{Operating System:} macOS, Version 13.4.1, Build Version: 22F770820d
\end{itemize}


\section{Accuracy Analysis}
\label{sec:accuracyAnalysis}
In terms of accuracy we compare the results of the plugin based system with the results of the jupyter notebook based system.
The results of the two systems should be the same, since the same datasets and hyperparameters are used.
Also, we should see the same results since the same actual code is executed in both systems, only the way of calling the code is different.
The results are compared by calculating the mean squared error between the two results.



\chapter{Discussion}
\label{chap:discussion}
\section{Achievements and Contribution}
\label{sec:achievementsAndContribution}
\section{Limitations}
\label{sec:limitations}


\chapter{Conclusion and Outlook}
\label{chap:zusfas}

\section{Outlook}

\printbibliography

All links were last followed on October 15, 2023.

\appendix

\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
