% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-
% !TEX root = ./main-english.tex

% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-side printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={Implementing Variational Quantum Algorithms as Compositions of Reusable Microservice-based Plugins},
  author={Matthias Weilinger},
  type=master,
  institute=iaas, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Prof.\ Dr.\ Dr.\ h.\ c.\ Frank Leymann},
  supervisor={M.Sc.\ Philipp Wundrack,\\M.Sc.\ Fabian Bühler},
  startdate={April 19, 2023},
  enddate={October 19, 2023}
]{scientific-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create picures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
  \section*{Kurzfassung}
  \section*{Abstract}


\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
  \listof{Listing}{List of Listings}

%mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
%Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
\listof{Algorithmus}{List of Algorithms}
%\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig

% Abkürzungsverzeichnis
\printnoidxglossaries

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{chap:introduction}

\chapter{Background}
\label{chap:background}

The study of \gls{qhana} and its advanced plugin interactions is rooted in a set of foundational principles that underpin its functionality.
This chapter aims to offer a comprehensive introduction to these principles, encompassing key areas such as optimization algorithms, quantum computing, \glspl{vqa}, \gls{rest}, and the core architecture of \gls{qhana}.
By delving into these foundational topics, readers will gain the necessary context to understand the innovative approaches adopted in this thesis.

\section{QHana}
\label{sec:qhana}

\gls{qhana} was conceived as a specialized application in the domain of Digital Humanities.
Its primary design offers users a platform to explore various machine learning algorithms on designated datasets.
While the primary vision of \gls{qhana} was to assess the potential advantages of quantum algorithms within the DH community, the rise and cloud availability of quantum computers further underscore its relevance and timeliness.

\gls{qhana} is designed to be extensible, allowing the integration of new data sources and quantum algorithms as plugins.
However, plugins are built for specific applications, limiting their reusability in other contexts.
Moreover, plugins for an application have to be developed in the same programming language as the application.
Even if a plugin can be reused in another application, its UI has to adapt to the new application, otherwise users may fail to understand the plugin's functionality.
To address this limitation, \gls{qhana} is built on a novel concept of Reusable Microservice-based Plugins (RAMPs).
This allows microservices with an integrated UI to be used as plugins by multiple applications, enhancing the reusability of the plugins across different applications \cite{Buehler2022}.

Central to this thesis is the integration of \glspl{vqa}
The envisioned approach allows for plugins to interact and collaborate, enabling the implementation of \glspl{vqa} as modular components within \gls{qhana}.
qhana has multiple parts ui, plugins, ... diagram
thesis is about plugins
\section{Quantum Computing}
\label{sec:quantumComputing}
Quantum computing is a cutting-edge field that exploits the principles of quantum mechanics to process information.
Unlike classical computers that use bits (0s and 1s) to store and process information, quantum computers use quantum bits, or \emph{qubits}.
Qubits, through the phenomena of superposition and entanglement, can exist in multiple states at once and be correlated with each other in ways that classical bits cannot \cite{Nielsen2010}.

Superposition allows a qubit to be in a state that is a combination of both 0 and 1, with a certain probability for each.
This property enables quantum computers to perform many calculations simultaneously, vastly increasing their potential computational power.
Entanglement, on the other hand, allows qubits that are entangled to be intimately linked regardless of the distance separating them.
A change in the state of one will instantaneously affect the state of the other, a phenomenon that Einstein famously called spooky action at a distance \cite{Einstein1935}.
This property is essential for many quantum algorithms, quantum error correction, and quantum teleportation, making it a fundamental resource in quantum information processing \cite{Nielsen2010,Preskill1998}.


\section{Variational Quantum Algorithms}
\label{sec:variationalQuantumAlgorithms}

\glspl{vqa} bring together the principles of quantum computing and optimization in a unique and powerful way.
They are a class of hybrid quantum-classical algorithms that leverage the strengths of both quantum and classical computing to solve complex problems \cite{McClean2016}.

The main concept of \glspl{vqa} is to use a sequence of quantum operations (a "quantum circuit") controlled by certain parameters.
These parameters are adjusted using classical optimization techniques with the aim of solving a specific problem.
This problem, in many cases, involves finding the lowest energy state, or "ground state", of a quantum system, a problem that maps to finding the minimum of a particular function \cite{Peruzzo2013}.

By leveraging classical optimization algorithms, \glspl{vqa} become more resistant to quantum errors, as the majority of the computation is performed on a classical computer.
This combination of quantum and classical resources makes \glspl{vqa} a promising type of algorithm for near-term quantum devices \cite{Moll2017}.


\section{Optimization Algorithms}
\label{sec:optimizationAlgorithms}
Optimization is a powerful tool that's ubiquitous in various scientific and technological domains.
At its core, optimization is about finding the best solution from a set of possible choices.
This section provides a snapshot of optimization's fundamental principles, paving the way for its deeper exploration in the context of \glspl{vqa} and finally microservice based \glspl{vqa}.

\subsection{Objective Functions}
\label{subsec:objectiveFunctions}
\glspl{of} are fundamental to optimization problems, underpinning a myriad of computational algorithms and models.
Depending on specific requirements, the aim might be to minimize or maximize these functions.
Notably, within the realm of \glspl{vqa}, the focus is primarily on function minimization \cite{Weinan2017}.

The core inputs to a \gls{of} typically encompass data points (denoted as $x$), corresponding labels or outcomes (represented by $y$), and a set of parameters or weights (often symbolized by $\theta$ or $w$).
These parameters dictate how the model responds to the input data and makes its predictions.
Additionally, certain \glspl{of} may also include hyperparameters as input, which control the behavior and complexity of the model.
In the context of optimization problems, the role of an \gls{of} is to capture both the problem we're attempting to solve and the strategy by which we're trying to solve it.
It provides a measure of the 'goodness' or 'fitness' of our current solution or parameters, and the aim is to adjust these parameters to improve this measure.

One example of an \gls{of} is the Lasso (Least Absolute Shrinkage and Selection Operator) Loss function.
The Lasso loss function is represented as:
\[
L(Y, X, W, \lambda) = ||Y - XW||^2_2 + \lambda ||W||_1
\]
In this equation:

\begin{itemize}
  \item \(Y\) is the vector of observed values.
  \item \(X\) is the matrix of input data points.
  \item \(W\) is the vector of weights, the parameters of the model.
  \item \(\lambda\) is the regularization parameter, a non-negative hyperparameter.
\end{itemize}

This function consists of two terms:
\begin{enumerate}
  \item The first term \(||Y - XW||^2_2\) is the mean squared error between the predicted and actual outcomes.
  It measures the discrepancy between the model's predictions and the true values.
  \item The second term \(\lambda ||W||_1\) is a regularization term, where \(||W||_1\) represents the L1 norm (sum of absolute values) of the weight vector.
  This term penalizes the absolute size of the coefficients, encouraging them to be small.
\end{enumerate}
The hyperparameter \(\lambda\) governs the trade-off between these two terms.
When \(\lambda = 0\), the \gls{of} reduces to ordinary least squares regression, and the weights are chosen to minimize the mean squared error alone.
As \(\lambda\) increases, more weight is given to the regularization term, and the solution becomes more sparse (i.e., more of the weights are driven to zero).
This can help to prevent overfitting by effectively reducing the complexity of the model \cite{ShalevShwartz2014}

\subsection{Minimization Functions}
\label{subsec:minimizationFunctions}
Minimization functions, often referred to as optimization algorithms, play a pivotal role in a vast array of computational models and algorithms.
In essence, they serve to iteratively enhance the parameters of a model to reduce the value of the \gls{of}.
The goal of these minimization functions is to find the optimal set of parameters that yield the lowest possible value of the \gls{of} within the constraints of the problem \cite{Nocedal2006}.

The process of optimization involves a search through the parameter space.
This search can be visualized as navigating a landscape of hills and valleys, with each point in the landscape corresponding to a different set of parameters, and the height at each point representing the value of the \gls{of}.
The goal of the minimization function is to find the lowest point in this landscape, corresponding to the minimum value of the \gls{of} \cite{Goodfellow2017}.

The core inputs to a minimization function are the initial parameters of the model or weights (denoted as \(\theta\) or \(w\)),
the \gls{of} that needs to be minimized, and the gradients of the \gls{of} with respect to the parameters.
Additionally, certain minimization functions may also include hyperparameters as input, which control the behavior and complexity of the optimization process \cite{Virtanen2020}.
For instance, the learning rate is a typical hyperparameter that determines the step size in each iteration of the optimization process.

There are numerous minimization functions used in computational problems, each with its own strengths and weaknesses.
These range from simple methods such as gradient descent, to more complex ones such as the Newton's method, stochastic gradient descent (SGD), RMSprop, and Adam.

One of the most fundamental and widely used minimization functions is the Gradient Descent.
To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.

The update rule of gradient descent is given as:

\[
\theta_{t+1} = \theta_t - \alpha \nabla F(\theta_t)
\]

In this formula:

\begin{itemize}
  \item \(\theta_{t+1}\) represents the parameters at the next time step.
  \item \(\theta_t\) represents the current parameters.
  \item \(\alpha\) is the learning rate, a positive scalar determining the size of the step.
  \item \(\nabla F(\theta_t)\) is the gradient of the \gls{of}.
\end{itemize}

Here, the \gls{of} \(F\) is assumed to be a differentiable function.
The gradient \(\nabla F(\theta_t)\) provides the direction of the steepest ascent at the point \(\theta_t\), and \(-\nabla F(\theta_t)\) provides the direction of steepest descent.
By taking a step in this direction, we move towards the minimum of the function.

The size of the steps taken is determined by the learning rate \(\alpha\), which is a hyperparameter that must be set before the learning process begins.
The learning rate controls how fast or slow we move towards the optimal weights.
If the learning rate is very large, we may skip the optimal solution.
If it is too small, we may need too many iterations to converge to the best values.

The choice of minimization function can significantly influence the efficiency and success of the optimization process.
While some minimization functions may perform well on certain problems, they may not yield similar results on others.
Therefore, understanding the underlying mechanisms of these functions and their suitability to the specific problem at hand is crucial.


\section{RESTful API Design}
In the evolving landscape of software design, microservices have emerged as a preferred architectural style, prized for their modularity, scalability, and independent deployability.
At the heart of \gls{qhana}'s design is a microservices-based plugin approach, which facilitates its dynamic and extensible nature.
Central to the orchestration of these microservices is the application of \gls{rest}ful API design.
\gls{rest} is an architectural style that sets forth constraints for creating web services.
\gls{rest}ful APIs, built upon these constraints, are pivotal in ensuring seamless communication between individual microservices, thereby enabling efficient data exchange and service integration.

For those seeking a deeper dive into the intricacies of \gls{rest}ful design in microservices architectures, influential works by Fielding \cite{Fielding2000} and practical insights by Richardson and Amundsen \cite{Richardson2013} are recommended.

\chapter{Problem Statement and Objectives}
\label{chap:problem}

Optimization algorithms, with their ability to find the best possible solution from a set of feasible solutions, play a pivotal role in numerous computational domains.
\glspl{vqa} are a subset of these algorithms that leverage quantum computing principles, particularly in the realm of \glspl{of} and minimization techniques.
However, the true potential of optimization, and by extension \glspl{vqa}, is often hindered by rigid platforms where the components of these algorithms are tightly integrated, limiting adaptability and innovation.

\gls{qhana}, with its unique environment tailored for experimenting with a myriad of machine learning and quantum algorithms, presents an opportunity to redefine this paradigm.
Yet, its current architecture does not fully exploit the modular benefits that can be achieved by decoupling the components of optimization algorithms.
Furthermore, while developing this modular framework, it's essential to allow for plugins to interact with each other.
This interaction-centric concept, once established, can be universally applied across \gls{qhana}, not just for optimization but for any scenario where plugin interaction is required.

\textbf{Problem Statement:}
How can we design and implement a modular framework within \gls{qhana} that allows components of optimization algorithms, specifically \glspl{of} and minimization functions, to be encapsulated as distinct, interchangeable plugins?
Furthermore, how can these plugins, especially in the context of \glspl{vqa}, be structured to communicate and collaborate seamlessly?

This problem encompasses several challenges:

\begin{itemize}
    \item \textbf{Communication:} Establishing a robust communication mechanism that enables interaction, data sharing, and collaboration among these plugins.
    \item \textbf{Interchangeability:} Designing a system where different \gls{of} and minimization plugins can be effortlessly swapped, ensuring adaptability in optimization and \glspl{vqa}.
    \item \textbf{Standardization:} Implementing a consistent interface for these plugins, ensuring uniformity and compatibility across various \gls{of} and minimization plugins.
    \item \textbf{User Experience:} Providing an intuitive environment where users can easily select, interchange, and experiment with different optimization components tailored to their needs and provide developers with an extensible framework to build new minimization and \gls{of} plugins.
\end{itemize}

Addressing this problem is essential to enhance the capabilities of \gls{qhana}, transforming it into a dynamic, adaptable, and user-centric platform for optimization and VQAs.
The subsequent sections of this thesis will delve into the methodologies, implementations, and evaluations related to this problem.

\chapter{Related Work}

\cite{Beisel2023} \cite{Beisel2023a} \cite{Thullier2021}

\chapter{Methodology}
\label{chap:methodology}
\section{Conceptual Framework}
\label{sec:conceptualFramework}

This section outlines the theoretical and conceptual groundwork that anchors the methodology for this thesis.
The framework is rooted in the principles of modularity, interactivity, and interchangeability in the context of plugin-based architectures for optimization algorithms in \gls{qhana}.

\subsection{Plugin-Based Architecture in QHana}
\gls{qhana}, with its design centered on the Digital Humanities, provides an extensible platform for experimenting with various algorithms.
\gls{qhana}'s architecture predominantly revolves around the concept of RAMPs.
The objective is to leverage \gls{qhana}'s inherent modularity by enabling components of optimization algorithms to function as distinct, interchangeable plugins.
This brings us to the significance of modularity in optimization.

\subsection{Significance of Modularity in Optimization}
When \glspl{vqa} are designed with modular components, it allows for increased flexibility.
Specifically, having distinct \glspl{of} and minimization functions means parts of the algorithm can be adjusted or replaced seamlessly.
This reflects the goals of modularity and flexibility intrinsic to \gls{qhana}'s design.

\subsection{Interactivity Between Plugins}
Interactivity in \gls{qhana} should encompasses various facets:

A plugin should be able to invoke the microfrontend of another plugin.
Control should revert to the invoking plugin once the invoked plugin completes its tasks.
Both short-running tasks and long-running tasks should be facilitated.
For the latter, a callback mechanism is proposed, wherein the invoking plugin can be notified upon the completion of a long-running task by the invoked plugin.

The need for such interactivity stems from the inherent nature of optimization.
The \glspl{of} and minimization must closely interact to produce meaningful optimization results.
Moreover, since an invoking plugin is unaware of the parameters or requirements of the invoked plugin, direct interaction becomes imperative for dynamic data exchange and collaborative processing.

Drawing inspiration from existing systems, the interaction between the minimization and \glspl{of} in implementations like SciPy's \emph{optimizer.minimize} \cite{Virtanen2020} function serves as a precedent.

To elevate the degree of interactivity between plugins within \gls{qhana}, this thesis introduces the innovative concept of \emph{interaction endpoints}.
While QHana already employs a metadata field for plugins known as \emph{entry points} - endpoints invoked by the \gls{qhana} \gls{ui} to render the \gls{ui} of a plugin - interaction endpoints extend this paradigm by marking specific endpoints as callable by other plugins.

A defining characteristic of an interaction endpoint is its \emph{type}.
Interaction endpoints sharing the same type uphold uniformity in their signature and return type.
This ensures that they are invoked consistently by other plugins, irrespective of their specific implementation

\subsection{Integration and Alignment with QHana}
The proposed approach complements \gls{qhana}'s existing principles, resonating with its emphasis on extensibility, adaptability, and user-centricity.
By enhancing the interactivity and modularity of plugins within \gls{qhana}, we strive to elevate its capabilities, making it a more dynamic platform for optimization and \glspl{vqa}.

\section{Architectural Design Strategy}
\label{sec:architecturalDesignStrategy}

The architectural design process is a critical phase that dictates how the components of the solution will function and interact with each other.
The methodology undertaken for the architectural design of the plugin-based optimization framework consisted of a series of well-defined steps, ensuring clarity, modularity, and extensibility.

\subsection{Decomposition into Plugins}
The first step involved decomposing the optimization process into distinct plugins.
This is achieved by reviewing relevant literature to understand common practices and methodologies in optimization \cite{Virtanen2020, Nocedal2006, ShalevShwartz2014, Weinan2017}.
The main challenge was to find a good decomposition so that on one hand all functionalities that should be interchangeable are encompassed in an extra plugin,
on the other hand too many plugins would create an overhead that would lead to inefficiency.

\subsection{Defining Plugin Responsibilities}
Once the plugins were identified, the next step was to precisely define the responsibilities of each plugin.
This ensured that every plugin had a well-defined purpose, preventing overlaps in functionality and ensuring clarity in their roles.
To delineate these responsibilities, several crucial decisions had to be made:

\begin{itemize}
  \item Which plugin is tasked with calculating the loss function?
  \item Which one handles the minimization process?
  \item Who prompts the user for the \gls{of} hyperparameters?
  \item Who gathers the minimization function hyperparameters from the user?
  \item Who inquires about the user's preference for the \gls{of} and minimization algorithm?
  \item Who solicits the input data from the user?
  \item Who requests the target variable?
  \item And lastly, who oversees and coordinates the entire process?
\end{itemize}

\subsection{Universal Plugin Interface Design}
Recognizing the vast possibilities and variations that interchangeable plugins might encompass, it was crucial to formulate a universally adhered-to interface.
This interface acted as a "contract", ensuring that irrespective of the specific implementation details of a plugin, there existed a consistent mode of interaction.
Core attributes and functionalities, like querying the number of initial wights for the minimization process required, were defined as part of this universal interface.
This approach fostered interchangeability and adaptability, as the defined interface could accommodate a multitude of interchangeable plugins, each with its unique functionalities.
In terms of optimization, this could mean that in case of an \gls{of} plugin, that is responsible for calculating the loss function, interfaces have to be generalized to allow for any thinkable type of loss function.


\subsection{Plugin Interaction Design}
The next step was to design the interaction between plugins.
This involved defining the various ways in which plugins could interact with each other.
It was crucial to build a robust and flexible interaction design that could accommodate a multitude of scenarios, since the interaction between plugins in \gls{qhana} should not be limited to the optimization process.
This interaction design should lay the foundation for all future plugin interactions within \gls{qhana}.

Scenarios that were considered included for instance a plugin invoking the microfrontend of another plugin, or it could call a specific endpoint of another plugin.
Additionally, the interaction design also encompassed the flow of control between plugins.
For instance, a plugin could invoke another plugin and then wait for it to complete its tasks before resuming its own tasks.
A plugin could also invoke another plugin and then stop execution all together, while providing the invoked plugin with a callback URL that the invoked plugin could call once it completes its tasks.
Alternatively, a plugin could invoke another plugin and then continue with its tasks without waiting for the invoked plugin to complete its tasks.
Intrinsic to this step was also the design of a coordination mechanism that facilitated the interaction between plugins.

\subsection{UML Diagrams}
To pictorially represent the responsibilities and interfaces of each plugin, component diagrams were formulated.
A diagram encapsulated the primary tasks of a plugin and the interfaces it provides, offering a clear understanding of the system's structure.

To fathom the intricate interplay between plugins, sequence diagrams were devised.
These diagrams capture the stepwise interaction between plugins.
They detail the flow of control and data, spotlighting the sequence of plugin invocations and collaborative processes.


\subsection{Feedback and Refinement}
Throughout the design process, continuous feedback loops were integrated. This involved:
\begin{itemize}
\item Revisiting each stage, evaluating its alignment with overarching goals.
\item Making necessary refinements to ensure the architecture is both robust and flexible.
\end{itemize}

With the architectural design strategy in place, the next step is to define the implementation strategy.

\section{Implementation Strategy}
\label{sec:implementationStrategy}

\subsection{Preliminary Analysis and Design}
Before diving into the implementation, an in-depth study of the \gls{qhana} documentation and a related paper on \gls{qhana}'s architecture \cite{Buehler2022} was undertaken to gain a comprehensive understanding of the system.
With component and sequence diagrams already in place, the next step was to define the implementation strategy.

\subsection{Development Environment and Toolset}

\begin{itemize}
    \item \textbf{Visual Studio Code}: A versatile integrated development environment employed for its adaptability and support for Python development, facilitating comprehensive coding, debugging, and testing for the project.

    \item \textbf{Docker}: Utilized to run all components of \gls{qhana}, ensuring consistent behavior across different computing environments.

    \item \textbf{Postman}: An API testing tool that was employed to validate and debug various endpoints, ensuring consistent and expected behavior of the plugin interactions.

    \item \textbf{Python}: As \gls{qhana} is implemented in Python, it was the primary language for backend development, offering versatility and a vast library ecosystem.

    \item \textbf{HTML}: Used for creating the microfrontends, forming the user interfaces of the plugins.

    \item \textbf{Flask}: A lightweight Python web framework employed to develop the web services and \gls{rest}ful APIs for the plugins.

    \item \textbf{Marshmallow}: A Python library pivotal for object serialization/deserialization, ensuring structured data transfer between the plugins.

    \item \textbf{Flask-Smorest}: An extension of Flask, offering tools for building \gls{rest}ful APIs with Flask and Marshmallow, ensuring structured and accurate data transfer between plugins.

    \item \textbf{Celery}: Integrated to manage long-running tasks, particularly for objective function plugins, allowing for asynchronous task execution.

    \item \textbf{Requests}: A Python library fundamental for facilitating interactions between plugins via HTTP requests.
\end{itemize}

\subsection{Key Principles of QHana Plugins}

In the \gls{qhana} ecosystem, plugins play a pivotal role in extending functionality.
The creation and integration of these plugins are governed by a set of principles that ensure their seamless operation and interaction.
For the context of this thesis, the following principles are deemed most significant:

\begin{itemize}
    \item \textbf{Plugin Definition:}
    A plugin is essentially a Python module or package.
    It contains a class that inherits from \texttt{QHAnaPluginBase} and should be situated in a directory specified by the \texttt{PLUGIN\_FOLDERS} configuration variable.
    The plugin runner imports all plugins from these directories, handling only the root module of the plugin.
    The plugin, in turn, is responsible for importing its implementation class and all associated Celery tasks.

    \item \textbf{Plugin Metadata and Endpoints:}
    Plugins should contain metadata and links to all their endpoints, typically located in the `./` directory.
    The metadata includes crucial information like entry points.

    \item \textbf{UI Interaction:}
    Plugins define both \texttt{href} and \texttt{hrefUi} to point to their micro frontend.
    The `hrefUi` serves the microfrontend where users input data, and `href` processes this input.

    \item \textbf{Data Handling in Multi-step Plugins:}
    For plugins that necessitate multiple user input steps, data is stored in a key-value store with dictionary-like functionality.
    Data specific to a plugin task is associated with a unique database ID, and subsequent endpoint URLs of a plugin with this ID follow the pattern \texttt{http(s)://…/<UUID>/endpointName}.

    \item \textbf{Handling Long Running Tasks:}
    Long-running tasks utilize Celery. Task names, incorporating the plugin name, must be unique.

    \item \textbf{File Loading from URLs:}
    Plugins are designed to load files from URLs.

    \item \textbf{Data Format Specification:}
    Data formats, especially those shared across plugins, should be defined as per \gls{qhana}'s guidelines.
    For instance, for the \texttt{text/csv} format pertaining to entities:
    \begin{itemize}
        \item The first column must be the ID column (named ID). Subsequent columns represent entity attributes.
        \item The CSV file should contain a header row specifying all attribute names.
    \end{itemize}
\end{itemize}

It's imperative to note that while the outlined principles are crucial for this thesis, \gls{qhana}'s overarching documentation provides a more exhaustive list of best practices and guidelines for plugin creation \cite{Buehler2022}.


\subsection{Data Handling and Transfer}
Flask-Smorest was instrumental in ensuring structured and accurate data transfer between plugins.
It aided in validating the correctness of passed data, returning appropriate error codes, and managing errors efficiently.
This ensured that data exchanges between plugins, especially in the context of \glspl{vqa}, were smooth and error-free.

\subsection{Testing and Debugging}

The quality and reliability of plugins and their interactions are paramount.
For this reason, a multi-faceted testing strategy was adopted:

\begin{enumerate}
    \item \textbf{Static Code Analysis}:
    \begin{itemize}
        \item \textbf{Purpose}: To ensure code quality, maintainability, and to identify potential vulnerabilities or deviations from coding standards.
        \item \textbf{Tools \& Implementation}: The tool \texttt{flake8} was utilized to conduct static code analysis on the Python codebase.
        By running \texttt{flake8}, a report detailing any code inconsistencies, potential errors, or areas for improvement was generated, providing valuable feedback for refinement.
    \end{itemize}

    \item \textbf{Logging and Monitoring}:
    \begin{itemize}
        \item \textbf{Purpose}: To capture, store, and analyze real-time information about the system's operations, thereby aiding in troubleshooting and understanding system behavior.
        \item \textbf{Tools \& Implementation}: Python's in-built \texttt{logging} package was leveraged to track and record various events during the execution of plugins.
        By strategically placing logging statements within the code, it was possible to gain insights into the flow of operations, detect anomalies, and pinpoint areas that might require attention.
    \end{itemize}

    \item \textbf{Interactive Debugging}:
    \begin{itemize}
        \item \textbf{Purpose}: To step through the code in real-time, inspect variables, and analyze the program's flow to identify and rectify issues.
        \item \textbf{Tools \& Implementation}: The integrated Python debugger in Visual Studio Code was employed.
        This debugger allowed for setting breakpoints, stepping through code, inspecting variable states, and examining the call stack, providing a granular view of the system's operations and aiding in issue identification and resolution.
    \end{itemize}

    \item \textbf{Manual Testing}:
    \begin{itemize}
        \item \textbf{Purpose}: To capture nuances and potential issues that might be overlooked.
        \item \textbf{Procedure}: A hands-on approach was adopted where the plugins were interactively used.
        This involved navigating through the \gls{ui}, experimenting with different inputs, and observing the system's reactions to ensure it behaved as expected and met user requirements.
    \end{itemize}
\end{enumerate}

By employing a combination of static code analysis, detailed logging, interactive debugging, and manual testing, it was ensured that the plugins were not only functionally correct but also adhered to coding standards and best practices, guaranteeing a robust and user-friendly experience.


\subsection{Performance Optimization}
For optimal performance in the microservice-based plugin architecture, several strategies were employed:

\begin{enumerate}
    \item Reducing the number of interactions between plugins.
    \item For each interaction, the amount of data transmitted across the network should be kept to a minimum to ensure efficient communication and faster response times.
    \item For tasks that require extended processing, the Celery framework should be utilized, allowing these tasks to operate asynchronously and thereby optimizing resource usage.
\end{enumerate}

These strategies were critical in ensuring swift and seamless interactions between plugins.


\subsection{Documentation and Extensibility}
To foster an environment of growth and encourage future development, comprehensive documentation detailing the process of adding new \gls{of} and minimization plugins was created.
This documentation serves as a guideline for developers aiming to extend the capabilities of \gls{qhana} with plugin interaction.


\section{Evaluation Strategy}
\label{sec:evaluationStrategy}

The implementation section of this thesis will ultimately present two distinct versions of a plugin-based optimization framework.
The evaluation will assess both versions to facilitate a comparison between the two approaches.
Additionally, the evaluation will juxtapose these approaches with a non-plugin-based approach, presented in the form of a Jupyter notebook.
To validate the effectiveness of the proposed solutions and to assess whether the goals set out in the problem statement have been achieved, the following evaluation methodologies are adopted:

\subsection{Performance Benchmarking}
Performance is paramount, especially in a plugin-based architecture.
A direct comparison will be made between the two plugin-based approaches and the non-service-based approach.
Key metrics for this comparison include:

\begin{itemize}
\item \textbf{Objective Function Calculation Time}: This measures the time taken to compute the loss, which directly impacts the overall optimization time.
For the service-based approach, the evaluation encompasses not only the calculation time of the \gls{of} but also the time taken for a call to the \gls{of} calculation endpoint.

\item \textbf{Minimization Time}: This refers to the time consumed to minimize the \gls{of}.
In the service-based approach, the time taken for the minimization endpoint to return a result is also accounted for.

\item \textbf{Network Latency}: This quantifies the time required for a request to reach the server and for the corresponding response to be received.
This metric is exclusive to the service-based approach.

\item \textbf{Network Traffic}: This represents the volume of data transmitted between the endpoints.
This metric is exclusive to the service-based approach.

\item \textbf{Database Access Times}: It is essential to gauge the time required to retrieve data from the database since endpoints access context data from the database during each invocation.
\end{itemize}

This evaluation hinges on quantitative metrics, with results graphically depicted for enhanced clarity.
The \emph{time.perf\_count} function from Python is employed to capture measurements, while the Google Chrome developer tools are used to measure network traffic.

\subsection{Interchangeability}
The true measure of interchangeability is the ability to swap components without causing disruptions.
Accordingly, every plugin of a particular type is tested against every other plugin of the same type to validate seamless interchangeability.

\subsection{Standardization Adherence}
Standardization, pivotal for ensuring compatibility and uniformity across diverse plugins, is put to the test.
An evaluation is undertaken to determine whether all implemented plugins conform to the prescribed standards.
This guarantees that all optimization plugins can be implemented uniformly, facilitating consistent and compatible integrations in the future.

\subsection{User Experience}
While the primary audience for this thesis is developers for the \gls{qhana} platform, the user experience is not to be undermined.
An assessment determines the ease and efficiency with which a developer can introduce a new plugin in the context of service-based optimization in \gls{qhana}.
The complexity and the lines of code required to integrate a new, interchangeable plugin for the optimization process are enumerated and compared across the approaches.

Through a meticulous evaluation across these parameters, this thesis endeavors to furnish a thorough appraisal of the solutions in relation to the challenges delineated in the problem statement.\section{Test Data Generation for Evaluation}

To robustly evaluate the solutions proposed in this work, it's essential to test them on a diverse range of datasets, reflecting both size and complexity.
The objective is to mimic real-world scenarios where optimization problems can range from simple tasks with a few data points to complex challenges with a vast number of features and data points.
The code that was used to generate the test data can be found in the appendix \ref{appendix:generate_data}.

\subsection{Dataset Characteristics:}

\begin{enumerate}
    \item \textbf{Variability in Size}: Datasets of different sizes have been generated, spanning from a modest 10 data points to a substantial 100,000 data points.
    This variation ensures that the optimization performance is assessed across different scales, from quick-to-process small datasets to the computationally demanding large datasets.

    \item \textbf{Features Determined by Size}: The number of features, \( x \), in each dataset is dynamically determined based on the dataset's size, specifically calculated as \( \text{int}(\sqrt{\text{size}} \times 1.5) \).
    This approach ensures that with the growth of the dataset, its complexity also increases, mirroring real-world scenarios where larger datasets often present more features or dimensions.

    \item \textbf{Synthetic Data with Noise}: The data is synthetically generated using the \texttt{make\_regression} function from the Scikit-learn library, known for its utility in machine learning.
    An added noise parameter introduces an element of randomness, making the optimization task more intricate and thereby closely resembling real-world challenges.

    \item \textbf{Ensuring Reproducibility}: To maintain the consistency and reliability of the generated datasets across multiple runs or evaluations, a fixed random seed (`np.random.seed(42)`) has been set.
    This ensures that the data, though synthetic and noisy, remains consistent across evaluations, enabling true comparisons, assessments and reproducibility.

    \item \textbf{Adherence to QHana Standards}:
   \begin{itemize}
        \item \textbf{Unique Entity IDs}: Each data point in the dataset is allocated a unique ID in the format 'entityX', where X represents the entity number. This aligns with \gls{qhana}'s data standard that mandates every data point to have an identifiable ID.
        \item \textbf{Data Format and Storage}: All datasets are stored in the CSV (Comma Separated Values) format, adhering to \gls{qhana}'s accepted data formats.
    \end{itemize}
\end{enumerate}

By employing such multifaceted and representative datasets, the methodology aims to guarantee a holistic evaluation of the optimization solutions, assessing their performance, interchangeability, and user experience across a diverse range of scenarios.

\chapter{Resulting System Architecture}
\label{chap:architecture}


\section{Resultant Decomposition into Plugins and their Responsibilities}

Following the decomposition strategy, the plugin-based optimization framework was divided into three primary plugin types:
\gls{of} plugin, the minimizer plugin, and the coordinator plugin.
Each of these plugins was equipped with specific roles and responsibilities, ensuring a modular and efficient optimization process.
This split is visualized in the component diagram in figure \ref{fig:component_diagram}.
It is important to note that for both proposed plugin-based approaches, this decomposition and the associated responsibilities remain consistent.

\subsection{Objective Function Plugin}

The \gls{of} plugin is central to the optimization framework, encapsulating the mathematical function that defines the problem at hand.
Its primary roles include:

\begin{itemize}
\item \textbf{Metadata Provision}: The plugin offers metadata about itself.
\item \textbf{Hyperparameters Acquisition}: It prompts the user to provide the hyperparameters necessary for the loss function calculation.
\item \textbf{Loss Calculation}: Based on the provided input data, the plugin computes the loss, representing the discrepancy between the predicted values and actual values.
\item \textbf{Gradient Calculation (Optional)}: For optimization algorithms that leverage gradient-based methods, the plugin can optionally compute the gradient of the loss function. This aids in guiding the optimization process towards the desired minima.
\end{itemize}

\subsection{Minimizer Plugin}

The minimizer plugin is responsible for iteratively adjusting parameters to minimize the loss provided by the Objective Function plugin.
Its functions include:

\begin{itemize}
\item \textbf{Metadata Provision}: Similar to the \gls{of} plugin, it provides essential metadata.
\item \textbf{Hyperparameters Acquisition}: The plugin acquires the hyperparameters crucial for the employed minimization algorithm from the user.
\item \textbf{Minimization Process}: Using the loss (and optionally the gradient) from the \gls{of} plugin, the minimizer plugin endeavors to find the parameter values that minimize this loss.
\end{itemize}

\subsection{Coordinator Plugin}

The Coordinator plugin acts as the orchestrator, ensuring seamless interaction between the \gls{of} and Minimization plugins and the user.
Its primary responsibilities are:

\begin{itemize}
\item \textbf{Plugin Selection}: It prompts the user to select the desired \gls{of} and minimizer plugins to be used in the optimization process.
\item \textbf{Data Acquisition}: The plugin gathers the necessary input data and the target variable from the user, which will be used in the optimization process.
\item \textbf{Endpoint Acquisition}: It obtains the necessary endpoints from the selected plugins, which will be used for interaction.
\item \textbf{Coordination Role}: It manages the interaction between the \gls{of} and minimizer plugins, ensuring that the loss (and optionally gradient) calculation function is provided to the minimizer plugin for the optimization process.
Additionally, it coordinates the interaction between the user and the selected plugins, ensuring the user is provided with the necessary microfrontends.
\item \textbf{Results Presentation}: Post-optimization, the coordinator plugin presents the optimization results to the user.
\end{itemize}


\section{Universal Plugin Interface Design}

The process of optimization is inherently complex, with a multitude of variations and nuances.
To ensure a streamlined interaction between different plugins, it was imperative to establish universal plugin interfaces for each type of plugin.
This interface acts as a standard that every plugin of a specific type has to adhere to.
The interfaces for each plugin are detailed below and are visualized in the component diagram in figure \ref{fig:component_diagram}.

% include component diagram from svg here with
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{graphics/plugin_decomposition.svg}
    \caption{Component Diagram of decomposed optimization process}
    \label{fig:component_diagram}
\end{figure}

\subsection{Objective Function Plugin Interfaces}

The \gls{of} plugin interface is designed to accommodate a wide range of loss functions, including those that require gradient computation.
Its interfaces are:

\begin{itemize}
\item \textbf{Metadata}: The plugin provides metadata about itself as specified in the \gls{qhana} documentation \cite{Buehler2022}.
\item \textbf{UIRef}: This endpoint is used to obtain the microfrontend for the \gls{of} plugin, which is used to collect the hyperparameters from the user.
This interface is also in accordance with the \gls{qhana} documentation \cite{Buehler2022}.
\item \textbf{HRef}: This endpoint is used to process the input from the \gls{of} microfrontend and is therefore usually called the \textbf{processing} endpoint.
It is usually triggered by the user clicking the \emph{submit} button on the microfrontend.
This interface is also in accordance with the \gls{qhana} documentation \cite{Buehler2022}.
\item \textbf{PassData}: This endpoint is used to pass the input data and the target data to the \gls{of} plugin.
It returns the number of weights that is required for the optimization process.
The number of weights is returned here since it is dependent on the data and possibly the hyperparameters.
\item \textbf{CalculateLoss}: This endpoint is used to calculate the loss.
\item \textbf{CalculateGradient (Optional)}: This endpoint is used to calculate the gradient of the loss function. It is optional since not all optimization algorithms require the gradient.
\item \textbf{CalculateLossAndGradient (Optinonal)}: This endpoint is used to calculate the loss and the gradient of the loss function. Similar to the previous endpoint, it is optional since not all optimization algorithms require the gradient.
\end{itemize}

\subsection{Minimizer Plugin Interfaces}

The minimizer plugin, responsible for optimizing the loss function provided by the \gls{of} plugin, offers these interfaces:

\begin{itemize}
  \item \textbf{Metadata}: same as for the \gls{of} plugin
  \item \textbf{UIRef}: same as for the \gls{of} plugin
  \item \textbf{HRef}: same as for the \gls{of} plugin
  \item \textbf{Minimize}: This endpoint is used to minimize the loss function.
\end{itemize}

\subsection{Coordinator Plugin Interfaces}

The Coordinator plugin, orchestrating the interaction between the OF and Minimizer plugins, is equipped with the standard \gls{qhana} interfaces \textbf{Metadata}, \textbf{UIRef}, and \textbf{HRef}.

By establishing these interfaces and ensuring that each plugin conforms to them, a systematic, consistent, and efficient optimization process is ensured.
This structured approach not only facilitates seamless interactions but also fosters interchangeability, modularity, extensibility and Interchangeability and making it easy to add new \gls{of} and minimizer plugins in the future.

\section{Plugin Interaction Design}

The design of plugin interactions is pivotal to ensuring efficient and seamless coordination between different components of the system.
Given the extensive possibilities of interactions within the \gls{qhana} environment, the design phase was meticulous, taking into consideration various scenarios and ensuring adaptability.
The two primary modes of interactions were short-running and long-running, each catering to specific requirements.

\subsection{Short-Running Interaction}

In short-running interactions, a plugin invokes another plugin's endpoint, typically via a `GET` or a `Post` request, and immediately receives a response.
This mode of interaction is synchronous, wherein the invoking plugin waits for the response before proceeding.
Instances of such interactions in the optimization context are:
\begin{itemize}
    \item The coordinator plugin retrieving metadata from the \gls{of} and minimizer plugins.
    \item The coordinator plugin passing data to the \gls{of} plugin.
    \item The minimizer plugin calls the \texttt{CalculateLoss} endpoint of the \gls{of} plugin.
\end{itemize}

\subsubsection{Long-Running Interaction}

For processes that demand extensive computation time or involve multiple steps, long-running interactions come into play.
In such a case the calling plugin invokes the endpoint of the invocable plugin.
This invoked endpoint then schedules a long-running task and returns immediately.
The calling plugin can then continue with its execution without waiting for the long-running task to finish.
A new endpoint for the calling plugin is defined that can be called by the long-running task once it is finished.
This endpoint is called a callback endpoint.
Instances of such interactions are:
\begin{itemize}
    \item The coordinator schedules the microfrontend of the \gls{of} plugin.
    The coordinator plugin provides a callback endpoint to the \gls{of} plugin that is called once the user entered the input and the input was processed.
    \item The coordinator schedules the microfrontend of the minimizer plugin.
    This mechanism works identical to the previous item.
    \item The coordinator plugin calls the minimization endpoint of the minimizer plugin.
    The minimizer plugin schedules the minimization process and returns immediately.
    Once the minimization process is finished, the minimizer plugin calls a callback endpoint of the coordinator plugin.
\end{itemize}
The design of these interactions ensures that regardless of the complexity or duration of tasks, plugins can interact seamlessly and efficiently.

\section{Introduction of Interaction Endpoints}
\label{sec:introie}

Building upon the idea of plugins interacting with each other, as detailed in the previous section, there arises a crucial question:
How does one plugin discover the available endpoints of another?
To address this very challenge, this thesis introduces a novel concept called \textit{interaction endpoints} to \gls{qhana}.

While \gls{qhana} already has a metadata field named `entry points', which are endpoints invoked by the \gls{qhana} UI to render a plugin's UI, interaction endpoints extend this idea further.
They specifically define endpoints in the metadata that other plugins can invoke, facilitating seamless integration and interaction among them.

The core of interaction endpoints is their \emph{type}.
All interaction endpoints with the same type must adhere to the same signature and return type.
This uniformity ensures that they can be interchangeably invoked by other plugins.
The \gls{of} plugin provides interaction endpoints of types \emph{calc\_loss}, \emph{calc\_grad}, \emph{calc\_loss\_and\_grad}, and \emph{of\_pass\_data}.
The minimizer plugin offers the `minimization' type.
These interaction endpoints correspond to the endpoints defined in the previous section.
The introduction of interaction endpoints significantly enhances the modularity and interchangeability within \gls{qhana}, paving the way for a more dynamic and adaptable plugin ecosystem.
More on how these interaction endpoints are implemented can be found in the implementation chapter \ref{chap:implementation}.

\section{Final Interaction Flow}
The architecture's final interaction flow can be broken down into three main parts, with each ending in a new \gls{ui} display to the user.
They are visualized in the sequence diagrams in figures \ref{fig:interaction_flow_part1}, \ref{fig:interaction_flow_part2}, and \ref{fig:interaction_flow_part3}.

The first part begins after selecting the optimization plugin and concludes when the \gls{of} plugin's \gls{ui} is set as the next step.
Here, the coordinator plugin retrieves the user-selected \gls{of} and minimization plugin metadata, including their interaction endpoints.
This flow is represented in Figure \ref{fig:interaction_flow_part1}.

The second part starts with the retrieval of the \gls{of} plugin's microfrontend and ends when the minimizer plugin's \gls{ui} set as the next step.
After the user inputs hyperparameters and submits, the \gls{of} processes the input and sends a callback to the coordinator plugin,
The callback endpoint then passes the input and target data to the PassData endpoint.
This is depicted in Figure \ref{fig:interaction_flow_part2}.

The last segment starts with the minimizer plugin's microfrontend retrieval and ends when the optimization process concludes.
After users input the minimization hyperparameters, the minimizer processes the data, and sends a callback to the coordinator.
The coordinator then triggers the minimization endpoint, initiating a long-running minimization task that continuously calls the \gls{of} calculation endpoint.
Once this task completes, a final callback is made to the coordinator with the results
This flow is shown in Figure \ref{fig:interaction_flow_part3}.

\newpage
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{graphics/interaction_flow_1.svg}
  \caption{Final interaction flow of the first part of the optimization process.}
  \label{fig:interaction_flow_part1}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{graphics/interaction_flow_2.svg}
  \caption{Final interaction flow of the second part of the optimization process.}
  \label{fig:interaction_flow_part2}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{graphics/interaction_flow_3.svg}
  \caption{Final interaction flow of the third part of the optimization process.}
  \label{fig:interaction_flow_part3}
\end{figure}



\chapter{Implementation}
\label{chap:implementation}

The resulting code that realized two different approaches to a plugin-based optimization framework can be found in \gls{qhana}'s GitHub repository\footnote{\url{https://github.com/UST-QuAntiL/qhana-plugin-runner}}.
As examples the code implements two types of minimizer plugins, the \emph{scipy} and \emph{scipy-grad} plugins, and three types of \gls{of} plugins, the \emph{ridge-loss}, \emph{hinge-loss}, and \emph{neural-network} plugins.
More on their implementation can be found in the following sections.
This chapter does not aim to provide a comprehensive overview of the code, but rather to highlight the key implementation details and differences between the two approaches.


\section{UI Creation for Plugin Interaction}
\label{sec:uiCreationForPluginInteraction}
The QHAna platform provides a streamlined mechanism to craft simple \glspl{ui} with input fields, generated from marshmallow schemas.
The primary step is to inherit from QHAna's \texttt{FrontendFormBaseSchema} class.
For the \gls{of} plugin, the input fields are the hyperparameters needed by the loss function.
For the \emph{ridge\-loss} plugin that would be the regularization parameter \emph{alpha} which schema is shown in Listing \ref{lst:ridge_loss_schema}.

An important input field for the coordinator plugin is the minimizer and \gls{of} plugin selection field.
This field is generated by the \texttt{PluginUrl} class, which is a custom class that extends marshmallow's \texttt{fields.Url} class.
It ensures that only plugins of the correct type can be selected.
This is achieved by checking a plugins' metadata for the correct label.
Imperative for this to work is that all minimizer plugins are labeled with \emph{minimization} and all \gls{of} plugins with \emph{objective-function}.
Listing \ref{lst:coordinator_schema} shows the minimizer plugin selection field in the coordinator plugin's \gls{ui}.

As the \glspl{ui} of the \gls{of} and minimizer plugins are designed to be invoked by the coordinator plugin, the \emph{UIHref} which serves the \gls{ui} needs to accept a callback URL.
For that reason all \emph{UIHref} endpoints of invocable plugins must include the \emph{CallbackUrlSchema} marshmallow schema as a query parameter.

\lsinputlisting[language=Python, caption=Ridge Loss Plugin UI Schema., label=lst:ridge_loss_schema]{code/hyperparameter_schema_ridge.py}
\lstinputlisting[language=Python, caption=Coordinator Plugin UI Schema., label=lst:coordinator_schema]{code/hyperparameter_schema_scipy.py}


\section{Invocation of Plugin Microfrontends by the Coordinator Plugin}
\label{sec:invocationOfPluginMicrofrontendsByTheCoordinatorPlugin}

As highlighted in the previous chapter, the coordinator plugin is responsible for invoking the microfrontends of both the \gls{of} and minimizer plugins.
To achieve this, a adaptation of \gls{qhana}'s \emph{add\_step} function was developed.
The \emph{add\_step} function is designed for multi-step plugins, calling another microfrontend of the same plugin after the user submits the current microfrontend.
The function was enhanced and renamed to \emph{invoke\_task}.
With this modification, the coordinator plugin can invoke another plugin's microfrontend by supplying the appropriate \emph{UIHref} and \emph{Href}.
Furthermore, the coordinator plugin has the capability to provide a callback URL to the invoked plugin.
This callback URL is incorporated as a query parameter within the \emph{Href} and \emph{UIHref} URLs.

\section{Callback to the Coordinator Plugin}
\label{sec:implementationOfCallbacksToTheCoordinatorPlugin}

The coordinator plugin requires callbacks from invocable plugins in two distinct scenarios:
\begin{enumerate}
    \item After the user submits the microfrontend of either the minimization or the \gls{of} plugin.
    \item Once the asynchronous minimization process concludes.
\end{enumerate}

For the first scenario, the process unfolds as follows:
\begin{itemize}
    \item The coordinator invokes the microfrontend of the invocable plugin using the \emph{invoke\_task} function, as elaborated in Section \ref{sec:invocationOfPluginMicrofrontendsByTheCoordinatorPlugin}.
    \item On user submission of the microfrontend, the callback URL is passed to the processing endpoint of the invocable plugin via a query parameter.
    \item Subsequently, the invocable plugin makes a post request to the aforementioned callback URL.
\end{itemize}

In contrast, the second scenario operates through the following mechanism:
\begin{itemize}
    \item The coordinator plugin initiates a post request to the minimizer plugin's minimization endpoint, embedding a callback URL within the body.
    \item The minimizer plugin schedules an asynchronous celery task for loss function minimization.
    It registers the callback URL in the database under the designation \emph{status\_changed\_callback\_urls}.
    This label is pivotal for subsequent retrieval of the callback URL.
    \item The endpoint returns, and the celery task commences the minimization process.
    \item Following the task's completion, its status updates through the already implemented \emph{save\_task\_result} function.
    This action now triggers a signal in case it finished or it ran into an error, courtesy of Python's \emph{blinker} library, indicating the status change and passing the database ID of the task as an argument.
    \item A dedicated signal handler retrieves the callback URL and the task view URL from the database.
    The handler then orchestrates a post request to the callback URL, embedding the task view URL within the body.
\end{itemize}
Listing \ref{lst:signal_handler} presents the implementation of the signal handler, while Listing \ref{lst:singal_emitter} illustrates the juncture where the signal is emitted.

\lstinputlisting[language=Python, caption=Signal Handler for Callbacks., label=lst:signal_handler]{code/signal_handler.py}
\lstinputlisting[language=Python, caption=Signal Emitter for Callbacks., label=lst:singal_emitter]{code/signal_emitter.py}



\subsection{Context between Plugin Endpoints}
\label{sec:contextBetweenPluginEndpoints}
To save the context between the endpoints of a plugin, the implementation uses the \gls{qhana} concept for multistep plugins.
A database task is created in the processing endpoint of the plugin that can be identified by a database ID.
All context data is saved in the database task e.g. the hyperparameters for the minimization process.
Endpoints that are called later, e.g. the minimization endpoint, can then retrieve the context data from the database task.
The ID of the database task is part of the URL of the endpoint.


\section{Implementation of Plugin Endpoints}
\label{sec:implementationOfInteractionEndpoints}

Interaction endpoints are the crux of the plugin-based optimization framework, by standardizing the inputs and outputs of interchable endpoints.
The implementation handles the standardized inputs and outputs with the help of marshmallow schemas.

\subsection{Interaction Endpoint Schemas}
The schemas are defined in the shared folder of the \gls{qhana} repository.
An input and an output schema is defined for each interaction endpoint type.
A plugin that want to invoke an interaction endpoint of another plugin can now fill the input schema and use the output schema to parse the response.
For the \gls{of} plugin the following interaction endpoints schemas are defined:
\begin{itemize}
  \item \textbf{pass\_data}: \emph{ObjectiveFunctionPassDataSchema} for the input and \emph{ObjectiveFunctionPassDataResponseSchema} for the output.
  \item \textbf{calc\_loss}: The schemas depend on whether the gradient is calculated or not. For simply calucating the loss the input schema is \emph{CalcLossOrGradInputSchema} and the output schema is \emph{LossResponseSchema}.
\end{itemize}
For the minimizers' \emph{minimization} interaction endpoint the input schema is \emph{MinimizerInputSchema}, while the output is simply an acknowledgement with the status code 200 indicating that the minimization process was started.
The other implementational challange is to make the endpoints of the plugins discoverable by other plugins.

\subsection{Custom Marshmallow Field for Data}
\label{sec:customMarshmallowFieldForData}
In order to facilitate the passing of input and target data between plugins, a custom marshmallow field was developed.
This field, named \emph{NumpyArray} and shown in \ref{lst:numpy_array_schema}, is designed to handle numpy arrays.
This field is able to handle numpy arrays of any dimensionality and any data type that is json serializable.
Not only is this field used for passing the data between endpoints but also for storing the data effectively in the database.
\lstinputlisting[language=Python, caption=Custom Marshmallow Field for Numpy Arrays., label=lst:numpy_array_schema]{code/numpy_array_schema.py}.

\subsection{Metadata Endpoint}
\label{sec:metadataEndpoint}
Interaction endpoints are defined in the metadata of the plugins.
Ad mentioned previously, the metadata endpoint of a plugin defined the \emph{EntryPoint} field which is up until now only used by the \gls{qhana} UI to find the microfrontend of a plugin.
This thesis extends the \emph{EntryPoint} field to include interaction endpoints.
An \emph{interactionEndpoints} field is a list that can contain multiple interaction endpoints.
Each interaction endpoint is a dictionary that contains the \textbf{type} and an \textbf{href} field.
The \textbf{type} field is a string that identifies the interaction endpoint type.
To make it easier for developers to find the correct type, the \emph{InteractionEndpointType} enum is defined in the shared folder.
The \textbf{href} field is a string that contains the URL of the interaction endpoint.
Listing \ref{lst:metadata} shows InteractionEndpoints field in the metadata of the \emph{ridge-loss} plugin.
\lstinputlisting[language=Python, caption=Metadata of the Ridge Loss Plugin., label=lst:metadata]{code/metadata.py}

The syntax of how the interaction endpoint is defined in the metadata changes slightly when the plugin is implemented as a multi-step plugin.
As mentioned in \ref{sec:contextBetweenPluginEndpoints}, the URL to an enpoint of a multi-step plugin contains the database ID of the task, that is only known at runtime.
This is reflected in the metadata by making the \textbf{href} field a format string that contains the database ID of the task.
Listing \ref{lst:metadata_multistep} shows the metadata of the \emph{ridge-loss} plugin, when it is implemented as a multi-step plugin.
On the other hand the calling plugin needs to replace the placeholder with the actual database ID of the task once it is known.
This difference already introduces the core difference between the two approaches, which is elaborated in the next section.
\lstinputlisting[language=Python, caption=Metadata of the Ridge Loss Plugin as a Multi-Step Plugin., label=lst:metadata_multistep]{code/metadata_multistep.py}

\section{Example Minimizers and Objective Functions}
\label{sec:exampleMinimizersAndObjectiveFunctions}
To demonstrate the plugin-based optimization framework, two minimizer plugins and three \gls{of} plugins were implemented.
The minimizer plugins are named \emph{scipy} and \emph{scipy-grad}, while the \gls{of} plugins are named \emph{ridge-loss}, \emph{hinge-loss}, and \emph{neural-network}.

\subsection{Minimizer Plugins}
\label{sec:minimizerPlugins}
The minimizer plugins are implemented as wrappers around the \emph{scipy.optimize.minimize} function.
The \emph{scipy} plugin uses this function to minimize the loss function without the gradient.
The input hyperparameter for this plugin is the minimization method, which are limited to the methods that do not require the gradient.
The \emph{scipy-grad} plugin uses the same function to minimize the loss function with the gradient.
The input hyperparameters for this plugin are all methods that require the gradient.
The output of both plugins is the same, namely the weights that minimize the loss function.

\subsection{Objective Function Plugins}
\label{sec:objectiveFunctionPlugins}
The \emph{ridge-loss} plugin implements the ridge loss function as demonstrated in \ref{alg:ridge_loss}, where the input hyperparameter is the regularization parameter \emph{alpha}.
The \emph{hinge-loss} plugin implements the hinge loss function as demonstrated in \ref{alg:hinge_loss}, where the input hyperparameter is the regularization parameter \emph{C}.
The \emph{neural-network} plugin implements a neural network with one hidden layer.
The input hyperparameters are the number of neurons in the hidden layer.
This plugin provides the option to calculate the gradient of the loss function, which can be used together with the \emph{scipy-grad} plugin.
The output of all three plugins is the loss value, while the \emph{neural-network} plugin also returns the gradient of the loss function.

\section{Differences between the Two Plugin-Based Implementation Approaches}
\label{sec:differencesBetweenTheTwoPluginBasedImplementationApproaches}
This implementation brought forward two different approaches to a plugin-based optimization framework.
This section highlights the key differences between the two approaches.

\subsection{First Approach}
\label{sec:firstApproach}
The key concept of the first appoach is that the plugins are as loosly coupled as possible, meaning that as few data as possible is passed between the plugins and the data isntead data should always be retrieved from the database if possible.
Version one of the implementation strictly adheres to this concept, especially when it comes to the \gls{of} plugin.
The \gls{of} plugin stores its hyperparameters in the database.
During the execution of the \emph{pass\_data} endpoint the input data and the target data is stored in the database.
That means that for the \emph{pass\_data} endpoint, where the number of input weights is calculated, the hyperparameters are retrieved from the database and only the input data and the target data are passed to the emdpoint.
The \emph{calc\_loss}, \emph{calc\_grad}, and \emph{calc\_loss\_and\_grad} endpoints only require the weights as input, since the hyperparameters and the data are already stored in the database.
For the minimer plugin that means that at no point it needs to know the hyperparameters of the \gls{of} or event the data.
But that also means that the database ID of the task has to be passed by the microfrontend callback process to the cooridnator plugin.
The coordinator plugin needs this ID to handle the interaction endpoints URLs as described in \ref{sec:metadataEndpoint}.

\section{Second Approach}
\label{sec:secondApproach}
The second approach is more tightly coupled than the first approach, when it comes to the \gls{of} plugin.
The \gls{of} plugin is not implemted as a multi-step plugin, meaning no context can be shared between the endpoints of the plugin, meaning that all data necessary in an enpoint has to be passed during the invocation of the endpoint.
That means that the microfrontend callback process has to pass the hyperparameters to the coordinator plugin, for later passing them to the \gls{of} plugin.
The coordinator plugin then passes the hyperparameters and the data to the \emph{pass\_data} endpoint of the \gls{of} plugin.
The minimizer plugin also has be aware of the hyperparameters and the data, since it now has to pass those to the \emph{calc\_loss} endpoint of the \gls{of} plugin.
To keep this process as generalized as possible, indepenent of the \gls{of}'s implementation, the hyperparameters are passed as a dictionary.
That way neither the cooditioner plugin nor the minimizer plugin has to know the exact hyperparameters of the \gls{of} plugin.
In return no database ID has to be passed by the microfrontend callback process to the coordinator plugin.



\section{Directory Structure and Plugin Loading}
\label{sec:directoryStructure}

\gls{qhana} maintains two primary directories for plugin management: the \textit{plugins} and the \textit{stable plugins} directories.
The former hosts plugins undergoing development, while the latter contains plugins that are deployment-ready.
Within these directories, individual plugins are neatly organized into dedicated subfolders.
Upon initiation, \gls{qhana} scans and loads plugins from these designated subfolders.

The focus of this thesis, the optimization plugin, resides in the \textit{plugins/optimization} directory.
To promote clarity and a structured layout, further divisions were made:

\begin{itemize}
  \item \textit{plugins/optimization/coordinator} – For the main optimization plugin.
  \item \textit{plugins/optimization/objective\_functions} – Where each \gls{of} plugin occupies its respective subfolder.
  \item \textit{plugins/optimization/minimizer} – Where each minimizer plugin occupies its respective subfolder.
\end{itemize}

Given that \gls{qhana}'s native architecture does not support the direct loading of plugins from nested subdirectories, a recursive plugin loader was developed for this purpose.
This loader traverses through the \textit{plugins} directory and its subdirectories.
The presence of an \textit{\_\_init\_\_.py} file within a folder acts as a confirmation for the plugin's legitimacy.
If any plugin needs to be excluded from the loading process, a \textit{.ignore} file is placed in its respective folder to act as a loading deterrent.

To ensure that the recursive process doesn’t compromise system performance, the recursion depth is capped at 4.
This threshold sufficiently accommodates the current plugin structure but can be adjusted upwards if future needs arise.

The folder hierarchy is further enriched by the inclusion of an \textit{interaction\_utils} directory, dedicated to housing utility functions for generalized plugin interactions.
Additionally, there's a \textit{shared} directory, which stores data structures and schemas utilized across the plugins related to optimization plugin.
A visual representation of the final folder structure, with all plugins implemented for this thesis, is shown in \cref{fig:folderStructure}.


\begin{figure}[h!]
  \dirtree{%
      .1 plugins.
      .2 optimizer.
      .3 coordinator.
      .3 interaction\_utils.
      .3 minimizer.
      .4 scipy\_minimizer.
      .4 scipy\_minimizer\_grad.
      .3 objective\_functions.
      .4 hinge\_loss.
      .4 neural\_network.
      .4 ridge\_loss.
      .3 shared.
  }
  \caption{QHana Plugin Folder Structure.}
  \label{fig:folderStructure}
\end{figure}


\chapter{Performance Results}
\label{chap:performanceResults}



\chapter{Evaluation}
\label{chap:evaluation}
\section{Sample Machine Learning Experiment}
\label{sec:sampleMachineLearningExperiment}
\section{Interchangeability of Plugins}
\label{sec:interchangeabilityOfPlugins}
\section{Performance Analysis}
\label{sec:performanceAnalysis}

The performance of the system is evaluated by measuring the time the system takes to complete a machine learning experiment.
The time that it takes for a user to input the data is not measured.
This benchmark should only measure the time that it takes for the system to complete the experiment.
We compare the performance of the plugin based system with the performance of a jupyter notebook based system.
There are several steps that are benchmarked in the plugin based system, since there is user interaction between the steps.
\begin{itemize}
  \item The time that it takes after the user has selected the plugins until the user can input the hyperparameters for the of plugin.
  \item The time that it takes after the user has input the of plugin hyperparameters until the user can input the hyperparameters for the minimizer plugin.
  \item The time that it takes after the user has input the minimizer plugin hyperparameters until the user gets the result of the experiment.
\end{itemize}
The times of those steps are summed up to get the total time that it takes for the plugin based system to complete the experiment.
The time that it takes for the jupyter notebook based system is measured from the start of the notebook until the user gets the result of the experiment.
The time is measured by adding a timestamp at the start of the step and at the end of the step.
The difference between the two timestamps is the time that it takes for the step to complete.
The time that it takes for the plugin based system to complete the experiment is compared to the time that it takes for the jupyter notebook based system to complete the experiment.
In order to get a more accurate result, the experiment is repeated several times and the average time is taken.
To make it as comparable as possible, the same datasets and hyperparameters are used for both systems.
The output file of the experiment should also have the same exact format.

Since the benchmark of the plugin based system shows worse performance than the jupyter notebook based system, we want to explore where the most time is spent.
Therefore, we measure the time it takes to call to calculate the loss function.
In case of the plugin based system this includes the network latency between the plugins.
We want to see how much time is spent in the network and how much time is spent in the actual calculation of the loss function.

For more sophisticated loss functions (in this case the neural network) we want to see how much time is spent for the setup of the neural network.
Since a microservice does not have state, the neural network has to be setup for every call of to calculate loss function.

\section{Hardware and System Specifications}

The benchmarks were conducted on a MacBook Pro. The detailed specifications of the machine are as follows:

\begin{itemize}
    \item \textbf{Model:} MacBookPro18,1
    \item \textbf{Processor (CPU):} Apple M1 Pro
    \item \textbf{Memory (RAM):} 32 GB (hw.memsize: 34359738368 bytes)
    \item \textbf{Graphics Processing Unit (GPU):}
    \begin{itemize}
        \item Chipset Model: Apple M1 Pro
        \item Type: GPU
        \item Bus: Built-In
        \item Total Number of Cores: 16
        \item Metal Support: Metal 3
    \end{itemize}
    \item \textbf{Operating System:} macOS, Version 13.4.1, Build Version: 22F770820d
\end{itemize}


\section{Accuracy Analysis}
\label{sec:accuracyAnalysis}
In terms of accuracy we compare the results of the plugin based system with the results of the jupyter notebook based system.
The results of the two systems should be the same, since the same datasets and hyperparameters are used.
Also, we should see the same results since the same actual code is executed in both systems, only the way of calling the code is different.
The results are compared by calculating the mean squared error between the two results.



\chapter{Discussion}
\label{chap:discussion}

Interaciton endpoints
this concept has several advantages
* it allows for full interchangeability of plugins
* it allows the developer that want to interact with a plugin to know what endpoints are available and how to call them
* an alternatiev would be to return the url of available endpoitns after passing through the ui, now we can completely skip the ui alltogether and just call the endpoint directly

\section{Achievements and Contribution}
\label{sec:achievementsAndContribution}
\section{Limitations}
\label{sec:limitations}


\chapter{Conclusion and Outlook}
\label{chap:zusfas}

\section{Outlook}

\printbibliography

All links were last followed on October 15, 2023.

\appendix
\lstinputlisting[language=Python, caption={Source code for generating a sample test dataset for benchmarking with 1000 samples, 47 features, 10 noise and 1 target}., label=lst:generate_test_dataset]{code/ridge_loss_data.py}.

\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
