% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-
% !TEX root = ./main-english.tex

% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-side printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={Implementing Variational Quantum Algorithms as Compositions of Reusable Microservice-based Plugins},
  author={Matthias Weilinger},
  type=master,
  institute=iaas, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Prof.\ Dr.\ Dr.\ h.\ c.\ Frank Leymann},
  supervisor={M.Sc.\ Philipp Wundrack,\\M.Sc.\ Fabian Bühler},
  startdate={April 19, 2023},
  enddate={October 19, 2023}
]{scientific-thesis-cover}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create picures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis
\ifdeutsch
  \section*{Kurzfassung}
\else
  \section*{Abstract}
\fi

<Short summary of the thesis>

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
\ifdeutsch
  \listof{Listing}{Verzeichnis der Listings}
\else
  \listof{Listing}{List of Listings}
\fi

%mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
%Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
\ifdeutsch
  \listof{Algorithmus}{Verzeichnis der Algorithmen}
\else
  \listof{Algorithmus}{List of Algorithms}
\fi
%\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig

% Abkürzungsverzeichnis
\printnoidxglossaries

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Notes what I have done so far}
\begin{itemize}
  \item Added recursive parsing of the plugin folders so that subfolders are also parsed
  \item created a callable plugin that gets the data parsed from its invoker via the database
  \item created a invoker that calls the callable plugin
  \item user can now select a plugin from the list of callable plugins, the list is narrowed down to the plugins that are compatible via the tag field
  \item creating a method to get the plugin name from the plugin URL
  \item made the callee plugin to multistep, to demonstrate that any amount of steps can be done in invoked plugin
  \item started with an optimizer plugin that gives the first frontend for the user to select the objective-function-plugin
  \item create an objective-function-plugin that takes means squared error as an objective function
  \item todo: creating a method to get the plugin metadata from the plugin URL (this is needed in order to get the entry points of the plugin)
  \item identified three key problems:
  \begin{itemize}
    \item Reliable way to pass a callback function to the callee plugin
    \item A way to get a list of interaction endpoints of the callee plugin
    \item A way to get the plugin metadata from the plugin URL
  \end{itemize}
  \item 19.05: I am currently working on a big problem
  \begin{itemize}
    \item I have the optimizer plugin which should call the objective-function-plugin so that it can ask the user for the hyperparameters of the objective function
    \item For that reason I need to add a next step as a celery task. 
    \item The thing is i don't want to handle the next task like other multistep plugins where they share a db id since the objective-function-plugin should be able to stand on its own.
    \item Therefore, I pass a callback function to the objective-function-plugin which it should call when it is done with the setup.
    \item works all fine like that
    \item The problem right now is how i call the objective-function-plugin from the optimizer plugin
    \item I need to add it as a step, which is usually done via the add\_step celery task
    \item This task though needs a db\_id which I don't want to add.
    \item When adding none it works to call the objective-function-plugin and its also possbile to call the callback function
    \item but the problem is that with the add\_step task the celery task is not called asyncronously
    \item therefore when the callback function is called the optimizer plugin is not yet finished with the add\_step task
    \item usually one would go in the called multistep and do a clear\_previous\_steps call to make sure that the previous steps are finished
    \item but this call does not work since the objective-function-plugin does not have a db\_id
    \item thinking right now....
    \item maybe we can finish the task in the callback function of the optimizer plugin?
    \item Still 19.05 here we are again
    \item I have now a working solution for the problem
    \item the proposed solution of clearing the previous steps in the callback function did work 
    \item the problem was that i forgot to add the call db\_task.save(cammit=True) in the callback function
    \item this is needed to save the changes to the database without it the cleared variable is not saved
  \end{itemize}
  \item 20.05: Today we give the objective-function-plugin the ability to calculate the loss function
  \begin{itemize}
    \item On callback we give the optimizer plugin the url of the calculateLoss function url
    \item The optimizer plugin then calls the calculateLoss function
  \end{itemize}
  \item 22.05: we continue with the upper
  \begin{itemize}
    \item it is now possible to calculate the loss function
    \item we simply call the calculation enpoint with a post request
    \item we pass all the necessary data in the body of the request
    \item we have a special schema for that
    \item current problem: passing the data to the minimize function of scipy.optimize
    \item we have the hyperparameters as a dict to keep it as generic as possible
    \item but the minimize function needs the hyperparameters as a list
    \item and also the loss function needs the hyperparameters as a dict, so lets see how we can solve this
    \item we maybe did it by just passing the hyperparameters as a dict to both of the functions 
    \item now we have the problem that the content type of the input file is not correctly set by postman
    \item we just change the code to accept the content type but lets not forget to change it back
  \end{itemize}
  \item 23.05: today we do some cleanup
  \begin{itemize}
    \item make the import relative so that it works with the docker container
    \item remove all no ops tasks
  \end{itemize}
  \item 02.06: 
  \begin{itemize}
    \item we want to skip the optimizer UI since we do not need any more input data
    \item tried to set the cleared value of the created step to true, but it did not work, since only the process step is started is by clicking submit in the ui
    \item trying to chain the tasks directly in the callback function, which works
  \end{itemize}
  \item 05.06:
  \begin{itemize}
    \item now I want to call the objective-function-plugin via the entry points that I get through the metadata, this is needed to make the plugins more generic
    \item I have a problem where the shared schemas can not be imported since the NumPy package cannot be found, added the NumPy package to the requirements for the opt plugin, but this did not solve the problems
    \item maybe this will be solved by creating a coordinator plugin that lives in the top level of the plugin folder
  \end{itemize}
  \item 12.06:
  \begin{itemize}
    \item Creating a top level coordinator plugin was not the solution. If an init file is present no further plugins will be loaded from the folder
    \item the shared schemas are now part of the coordinator plugin
    \item Created a new infrastructure for the plugins with a coordinator plugin (diagram will be needed)
    \item today we solve the following problem:
    \item coordinator waits for the optimizer plugin to finish and writes the result to a file
    \item for this the coordinator polls the task api to check if the minimizer task is finished
    \item when it is it writes the result to a file
    \item I should move the callback URL away from the query parameters to the body of the request, maybe to the form
    \item next I should read about how neural networks work and how to minimize them
  \end{itemize}
  \item 13.06:
  \begin{itemize}
    \item i have made a decision on how to handle the callback url from the ui to the processing endpoint
    \item until now the callback url was passed as a query parameter
    \item now i want to pass it into the form that is rendered by the UI
    \item it should be a hidden field
    \item i have to make and change so that i can pass multiple schemas to the render function and set which fields should be hidden
  \end{itemize}
  \item 16.06:
  \begin{itemize}
    \item when an invoked plugin now makes a callback to its invoker it only passes back the endpoint for the calculation endpoint. 
    \item it does not pass any hyperparameters back since it should own the hyperparameters and not the invoker
    \item i now want to have a look of how loss functions are called in python and how hyperparameters are passed to them
    \item with this information I want to create interaction endpoints for the objective-function-plugin that are completely generic, i.e. the hyperparameters are passed to the endpoint as a dict
    \item as an example of how a generic method could look like i will have a look at the scipy.optimize.minimize function
    \item I now interaction endpoints as and additional list to entry points
  \end{itemize}
  \item 25.06:
  \begin{itemize}
    \item i have added a functionality where I use the blinker library to create a signal that is emitted when the status of a task changes
    \item this is used by the coordinator plugin to check if the minimization task is finished
    \item the coordinator plugin passes the callback URL to the minimization calculation endpoint
    \item this endpoint registers the URL to the db
    \item the signal handlers makes a post request to this URL when the status of the task changes
    \item I have moved all shared schemas into a separate folder
    \item I have moved all utilities concerning plugin interactions into a separate folder
  \end{itemize}
  \item 26.06:
  \begin{itemize}
    \item I developed a way to pass any amount of parameters to the processing endpoint of the invoked plugin UI
    \item I added an interaction endpoint that is used to invoke the invoked plugin
    \item this interaction endpoint allows any number of parameters
    \item the endpoint saves these parameters to the database
    \item it then adds the entry points of the invoked plugin as a step
    \item it passes the processing URL with the db id as a query parameter to the invoked plugin
    \item the invoked plugin then calls the processing endpoint with the db id as a query parameter
    \item here it starts a new db task with the arguments that were passed to the invoked plugin 
  \end{itemize}
\end{itemize}


\chapter{Define the plugins}
\begin{itemize}
  \item \textbf{ObjectiveFunction}: This plugin should have the following steps
  \begin{itemize}
    \item \textbf{ /get hyperparameterUI }: This step should let the user select the hyperparameters of the objective function
    \item \textbf{ /post ObjectiveFunctionSetup}: This step should setup the objective function with set a database id for future reference of the parameter. 
    Then it should store the following information to the database:
    \begin{itemize}
      \item hyperparameters
      \item more stuff?? %%% TODO specify what more stuff
    \end{itemize}
    Then it should call the optimizer callback function to indicate that the setup is done. Pass the url of the calculateLoss function as a parameter.
    \item \textbf{ /post CalculateLoss (dbID) }: this step should trigger the calculation of the loss function and should return it.
  \end{itemize}
  \item \textbf{Optimizer}: This plugin should have the following steps:
    \begin{itemize}
      \item \textbf{ /get setup UI }: Let the user select the objective-function-plugin, dataset, target variable, and the optimization algorithm
      \item \textbf{ /post setup }: This step should setup the optimizer with set a database id for future reference of the parameter. 
      Then it should call the objective function entry point to setup the objective function. Pass the url of the optimizer callback function as a parameter.
      \item \textbf{ /post callback }: This endpoint should be called by the objective-function-plugin to indicate that the setup is done.
      It should then start the optimization process.
      \item \textbf{ /post optimize }: This step should trigger the optimization process.
      It should loop the optimization function until a stop condition is met.
      In each iteration it should call the objective-function-plugin to calculate the loss function.
    \end{itemize}
\end{itemize}

\chapter{Introduction}

This thesis starts with \cref{chap:k2}.

We can also typeset \verb|<text>verbatim text</text>|.
Backticks are also rendered correctly: \verb|`words in backticks`|.

\chapter{Background}
\label{chap:literatureReview}
\section{Optimization Algorithms}
\label{sec:optimizationAlgorithms}
\subsection*{Objective Functions}
\label{subsec:objectiveFunctions}
OFs, serving as the cornerstone of optimization problems, form the foundation for a wide range of computational algorithms and models.
They provide a metric to gauge the performance of a given model, solution, or set of parameters.
Objective functions are the heart of many optimization problems.
The goal is to minimize or maximize these functions depending on the context and requirements \cite{Weinan2017}.
In the context of VQAs, the primary objective is to minimize the function.

The core inputs to an of typically encompass data points (denoted as $x$), corresponding labels or outcomes (represented by $y$), and a set of parameters or weights (often symbolized by $\theta$ or $w$).
These parameters dictate how the model responds to the input data and makes its predictions.
Additionally, certain ofs may also include hyperparameters as input, which control the behavior and complexity of the model.
In the context of optimization problems, the role of an of is to capture both the problem we're attempting to solve and the strategy by which we're trying to solve it.
It provides a measure of the 'goodness' or 'fitness' of our current solution or parameters, and the aim is to adjust these parameters to improve this measure.

One example of an of is the Lasso (Least Absolute Shrinkage and Selection Operator) Loss function.
The Lasso loss function is represented as:
\[
L(Y, X, W, \lambda) = ||Y - XW||^2_2 + \lambda ||W||_1
\]
In this equation:
- \(Y\) is the vector of observed values.
- \(X\) is the matrix of input data points.
- \(W\) is the vector of weights, the parameters of the model.
- \(\lambda\) is the regularization parameter, a non-negative hyperparameter.

This function consists of two terms:
\begin{enumerate}
  \item The first term \(||Y - XW||^2_2\) is the mean squared error between the predicted and actual outcomes.
  It measures the discrepancy between the model's predictions and the true values.
  \item The second term \(\lambda ||W||_1\) is a regularization term, where \(||W||_1\) represents the L1 norm (sum of absolute values) of the weight vector.
  This term penalizes the absolute size of the coefficients, encouraging them to be small.
\end{enumerate}
The hyperparameter \(\lambda\) governs the trade-off between these two terms.
When \(\lambda = 0\), the of reduces to ordinary least squares regression, and the weights are chosen to minimize the mean squared error alone.
As \(\lambda\) increases, more weight is given to the regularization term, and the solution becomes more sparse (i.e., more of the weights are driven to zero).
This can help to prevent overfitting by effectively reducing the complexity of the model \cite{ShalevShwartz2014}

\subsection*{Minimization Functions}
\label{subsec:minimizationFunctions}
Minimization functions, often referred to as optimization algorithms, play a pivotal role in a vast array of computational models and algorithms.
In essence, they serve to iteratively enhance the parameters of a model to reduce the value of the of.
The goal of these minimization functions is to find the optimal set of parameters that yield the lowest possible value of the of within the constraints of the problem \cite{Nocedal2006}.

The process of optimization involves a search through the parameter space.
This search can be visualized as navigating a landscape of hills and valleys, with each point in the landscape corresponding to a different set of parameters, and the height at each point representing the value of the of.
The goal of the minimization function is to find the lowest point in this landscape, corresponding to the minimum value of the of \cite{Goodfellow2017}.

The core inputs to a minimization function are the initial parameters of the model or weights (denoted as \(\theta\) or \(w\)),
the of that needs to be minimized, and the gradients of the of with respect to the parameters.
Additionally, certain minimization functions may also include hyperparameters as input, which control the behavior and complexity of the optimization process \cite{Virtanen2020}.
For instance, the learning rate is a typical hyperparameter that determines the step size in each iteration of the optimization process.

There are numerous minimization functions used in computational problems, each with its own strengths and weaknesses.
These range from simple methods such as gradient descent, to more complex ones such as the Newton's method, stochastic gradient descent (SGD), RMSprop, and Adam.

One of the most fundamental and widely used minimization functions is the Gradient Descent. 
To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. 

The update rule of gradient descent is given as:

\[
\theta_{t+1} = \theta_t - \alpha \nabla F(\theta_t)
\]

In this formula:

\begin{itemize}
  \item \(\theta_{t+1}\) represents the parameters at the next time step.
  \item \(\theta_t\) represents the current parameters.
  \item \(\alpha\) is the learning rate, a positive scalar determining the size of the step.
  \item \(\nabla F(\theta_t)\) is the gradient of the of.
\end{itemize}

Here, the of \(F\) is assumed to be a differentiable function.
The gradient \(\nabla F(\theta_t)\) provides the direction of the steepest ascent at the point \(\theta_t\), and \(-\nabla F(\theta_t)\) provides the direction of steepest descent.
By taking a step in this direction, we move towards the minimum of the function.

The size of the steps taken is determined by the learning rate \(\alpha\), which is a hyperparameter that must be set before the learning process begins.
The learning rate controls how fast or slow we move towards the optimal weights.
If the learning rate is very large, we may skip the optimal solution.
If it is too small, we may need too many iterations to converge to the best values.

The choice of minimization function can significantly influence the efficiency and success of the optimization process.
While some minimization functions may perform well on certain problems, they may not yield similar results on others.
Therefore, understanding the underlying mechanisms of these functions and their suitability to the specific problem at hand is crucial.

\section{Quantum Computing}
\label{sec:quantumComputing}
Quantum computing is a cutting-edge field that exploits the principles of quantum mechanics to process information.
Unlike classical computers that use bits (0s and 1s) to store and process information, quantum computers use quantum bits, or "qubits."
Qubits, through the phenomena of superposition and entanglement, can exist in multiple states at once and be correlated with each other in ways that classical bits cannot \cite{Nielsen2010}.

Superposition allows a qubit to be in a state that is a combination of both 0 and 1, with a certain probability for each.
This property enables quantum computers to perform many calculations simultaneously, vastly increasing their potential computational power.
Entanglement, on the other hand, allows qubits that are entangled to be intimately linked regardless of the distance separating them.
A change in the state of one will instantaneously affect the state of the other, a phenomenon that Einstein famously called "spooky action at a distance" \cite{Einstein1935}.
This property is essential for many quantum algorithms, quantum error correction, and quantum teleportation, making it a fundamental resource in quantum information processing \cite{Nielsen2010,Preskill1998}.


\section{Variational Quantum Algorithms}
\label{sec:variationalQuantumAlgorithms}

VQAs bring together the principles of quantum computing and optimization in a unique and powerful way.
They are a class of hybrid quantum-classical algorithms that leverage the strengths of both quantum and classical computing to solve complex problems \cite{McClean2016}.

The main concept of VQAs is to use a sequence of quantum operations (a "quantum circuit") controlled by certain parameters.
These parameters are adjusted using classical optimization techniques with the aim of solving a specific problem.
This problem, in many cases, involves finding the lowest energy state, or "ground state", of a quantum system, a problem that maps to finding the minimum of a particular function \cite{Peruzzo2013}.

By leveraging classical optimization algorithms, VQAs become more resistant to quantum errors, as the majority of the computation is performed on a classical computer.
This combination of quantum and classical resources makes VQAs a promising type of algorithm for near-term quantum devices \cite{Moll2017}.

\section{QHana}
\label{sec:qhana}


\section{Restful API Design}


\chapter{Related Work}

Describe relevant scientific literature related to your work.

\chapter*{Methodology}
\label{chap:methodology}
\section*{Concept Development for Plugin Interaction}
\label{sec:conceptDevelopment}

\chapter{Implementation}
\label{chap:implementation}
\section{Plugin Infrastructure}
\label{sec:pluginInfrastructure}
\section{Plugin Interaction}
\label{sec:pluginInteraction}
\section{Minimizer Plugin}
\label{sec:minimizerPlugin}
\section{Objective Function Plugins}
\label{sec:objectiveFunctionPlugins}


\chapter{Evaluation}
\label{chap:evaluation}
\section{Sample Machine Learning Experiment}
\label{sec:sampleMachineLearningExperiment}
\section{Interchangeability of Plugins}
\label{sec:interchangeabilityOfPlugins}
\section{Performance Analysis}
\label{sec:performanceAnalysis}

\chapter*{Discussion}
\label{chap:discussion}
\section*{Achievements and Contribution}
\label{sec:achievementsAndContribution}
\section*{Limitations}
\label{sec:limitations}


\chapter{Conclusion and Outlook}
\label{chap:zusfas}

\section*{Outlook}

\printbibliography

All links were last followed on March 17, 2018.

\appendix

\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
