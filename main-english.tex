% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-
% !TEX root = ./main-english.tex

% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse
\let\ifenglisch\iftrue
\input{pre-documentclass}
\documentclass[
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-side printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={Implementing Variational Quantum Algorithms as Compositions of Reusable Microservice-based Plugins},
  author={Matthias Weilinger},
  type=master,
  institute=iaas, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Informatik},
  examiner={Prof.\ Dr.\ Dr.\ h.\ c.\ Frank Leymann},
  supervisor={M.Sc.\ Philipp Wundrack,\\M.Sc.\ Fabian Bühler},
  startdate={April 19, 2023},
  enddate={October 19, 2023}
]{scientific-thesis-cover}
\usepackage{ifoddpage}

\input{acronyms}

\makeindex

\begin{document}

%tex4ht-Konvertierung verschönern
\iftex4ht
  \Configure{$}{\PicMath}{\EndPicMath}{}
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
\pagenumbering{arabic}
\Titelblatt

%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis

\section*{Abstract}
Quantum computing, with its transformative processing capabilities, has ushered in a new computational era, presenting both unparalleled opportunities and intricate challenges.
One significant beneficiary of this quantum revolution is the \gls{dh}, which has added a quantitative dimension to its traditionally qualitative realm.
\gls{qhana}, originally designed specifically for \gls{qdh}, emerges as a pivotal system in this context.
This thesis focuses on enhancing \gls{qhana} by integrating variational algorithms, especially \glspl{vqa}, in a modular manner.
The objective is to encapsulate components of variational algorithms as distinct, interchangeable plugins, ensuring adaptability and user-centricity.
Addressing challenges like robust plugin communication and intuitive user experience, the research delves into the design, implementation, and evaluation of this modular framework.
Beyond the immediate application to \glspl{vqa}, the insights and methodologies derived here lay foundational groundwork for future modular system designs in the quantum computing domain.


% \section*{Kurzfassung}
% Quantum computing, mit ihren transformativen Verarbeitungsfähigkeiten, hat eine neue Ära der Berechnung eingeläutet und bietet sowohl noch nie dagewesene Chancen als auch komplexe Herausforderungen.
% Ein bedeutender Profiteur dieser Quantenrevolution ist die \gls{dh}, die durch digitale Werkzeuge ihrer traditionell qualitativen Domäne eine quantitative Dimension hinzugefügt hat.
% \gls{qhana}, speziell für \gls{qdh} entwickelt, tritt in diesem Kontext als ein zentrales System hervor.
% Diese Arbeit konzentriert sich auf die Verbesserung von \gls{qhana} durch die Integration von variationalen Algorithmen, insbesondere \glspl{vqa}, auf modulare Weise.
% Das Ziel ist es, Komponenten von variationalen Algorithmen als eigenständige, austauschbare Plugins zu kapseln, um Anpassungsfähigkeit und Benutzerzentriertheit zu gewährleisten.
% Die Forschung geht auf Herausforderungen wie robuste Plugin-Kommunikation und intuitive Benutzererfahrung ein und vertieft das Design, die Implementierung und die Bewertung dieses modularen Frameworks.
% Jenseits von \gls{vqa} legen die Ergebnisse eine Grundlage für das modulare Systemdesign in \gls{qdh}.

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables

%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
\lstlistoflistings
%ansonsten:
  % \listof{Listing}{List of Listings}

%mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
%Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
% \listof{Algorithmus}{List of Algorithms}
%\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig

% Abkürzungsverzeichnis
\printnoidxglossaries

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{chap:introduction}

The dawn of quantum computing has revolutionized the landscape of computational research.
With its unparalleled processing capabilities and the potential to solve problems deemed undoable for classical computers, quantum computing heralds a new era of possibilities \cite{Shor}.
However, with these possibilities come challenges that demand innovative solutions, particularly in harnessing the power of quantum algorithms for diverse applications.

One domain that stands to benefit immensely from the quantum revolution is the \gls{dh}.
Traditionally, the humanities have been viewed through a lens of qualitative analysis.
However, with the advent of digital tools and methodologies, a quantitative dimension has emerged, enabling researchers to analyze vast datasets, uncover patterns, and derive insights that were previously unavailable \cite{Barzen2019}.
With its inherent strengths, Quantum computing offers the potential to elevate the \glspl{dh} to new heights, enabling analyses of unprecedented complexity and depth \cite{Barzen2022}.

Enter \gls{qhana} \cite{Buehler2022}.
Initially conceived and meticulously crafted for \gls{qdh}, its applicability has evolved over time, proving valuable not just for \glspl{dh} but for broader quantum computing applications.
While its core architecture is robust and versatile, there lies an untapped potential: integrating variational algorithms, specifically \glspl{vqa} in a modular fashion.
The vision is ambitious yet clear - components of variational algorithms encapsulated as distinct, interchangeable plugins within \gls{qhana}.
Such a modular approach promises enhanced adaptability and a user-centric platform where researchers can precisely tailor their quantum optimization strategies.

However, the path to realizing this vision is full of challenges.
Integrating optimization as multiple plugins demands a robust communication mechanism, ensuring seamless interaction and data sharing.
Furthermore, the user experience must remain intuitive, allowing both quantum novices and experts to harness the full potential of the modular quantum optimization framework.

The thesis embarks on a journey to navigate challenges and aims to design and implement a modular framework within \gls{qhana}. Delving into the intricacies of plugin-based architectures, the nuances of quantum optimization algorithms, and the challenges of ensuring seamless interaction between distinct components.

Moreover, the implications of this research extend beyond the limits of \gls{vqa} in \gls{qhana}.
By pioneering a methodology for \gls{vqa} plugin interactions within \gls{qhana}, this work lays the foundation for a broader paradigm of modular system design in the \gls{qdh}.

The thesis undertakes a comprehensive journey through modular \glspl{vqa} within \gls{qhana}.
In Chapter \ref{chap:background}, the groundwork is laid by delving into the foundational principles and concepts that underpin the entire thesis.
Chapter \ref{chap:problem} sharpens the focus, presenting a clear and concise problem statement and outlining the objectives aimed to be achieved.
With a clear understanding of the problem at hand, Chapter \ref{chap:methodology} dives deep into the methodology.
Here, the strategies and approaches employed to develop and evaluate the modular framework within \gls{qhana} are explored.
Chapter \ref{chap:architecture} unveils the architectural blueprint of the modular framework.
Moving from design to action, Chapter \ref{chap:implementation} delves into the details of the framework's implementation.
In Chapter \ref{chap:results}, the design and implementation are put to the test. The architectural design's efficacy and real-world implementation are measured through rigorous evaluations, drawing insights from the results.
Chapter \ref{chap:discussion} delves into a thorough analysis of the findings and checks if set problems are adequately solved.
To place the work in a broader context, Chapter \ref{chap:relatedWork} explores the landscape of related research.
Finally, Chapter \ref{chap:conclusion} brings the journey to an end, summarizing the resulting framework and looking ahead to future possibilities and roads for further extension.


\chapter{Background}
\label{chap:background}

The study of \gls{qhana} and its advanced plugin interactions is rooted in foundational principles underpinning its functionality.
This chapter aims to offer a comprehensive introduction to these principles, encompassing key areas such as optimization algorithms, quantum computing, \glspl{vqa}, \gls{rest}, and the core architecture of \gls{qhana}.
By delving into these foundational topics, readers will gain the necessary context to understand the innovative approaches adopted in this thesis.

\section{QHana}
\label{sec:qhana}

\gls{qhana} was conceived as a specialized application in the domain of \gls{dh} and has since evolved into a versatile platform for quantum computing applications.
Its primary design allows users to explore various machine-learning algorithms on designated datasets.
While the primary vision of \gls{qhana} is to assess the potential advantages of quantum algorithms within the \gls{dh} community, the rise and cloud availability of quantum computers \cite{Castelvecchi2017} further underscore its relevance and timeliness.

\gls{qhana} is designed to be extensible, allowing the integration of new data sources and quantum algorithms as plugins.
However, plugins are built for specific applications, limiting their reusability in other contexts.
Moreover, an application's plugins must be developed in the same programming language as the application.
Even if a plugin can be reused in another application, its \gls{ui} has to adapt to the new application.
Otherwise, users may need help understanding the plugin's functionality.
To address this limitation, \gls{qhana} is built on a novel concept of \glspl{ramp}.
This concept allows microservices with an integrated \gls{ui} to be used as plugins by multiple applications, enhancing the reusability of the plugins across different applications \cite{Buehler2022}.

\subsection{QHAna's Architecture}
\label{subsec:qhanaArchitecture}
\gls{qhana}'s architecture, as depicted in Figure \ref{fig:qhanaarch}, is characterized by its \gls{ui} and backend components.
The \gls{ui} offers users platform to navigate and interact with the system's functionalities, including the integrated quantum algorithms.
It acts as the primary touchpoint, bridging users with the system's underlying computational capabilities and data sources.

Conversely, the backend stands as QHAna's computational core, orchestrating the execution of quantum algorithms, managing data sources, and ensuring fluid communication with the \gls{ui}.
Its modular design facilitates the integration of new algorithms and data sources, adapting to the evolving needs of the \gls{dh} community.

Integral to QHAna's extensible architecture are the \glspl{ramp}.
These components not only enhance the system's capabilities but also ensure that their \gls{ui} elements cohesively integrate with the primary application interface, fostering a unified user experience.

\subsection{RAMPs: Bridging UI and Data Processing}
\label{subsec:ramps}

At their core, \glspl{ramp} are designed to serve as plugins, but they go beyond traditional plugin functionalities by offering a more holistic approach to both user interaction and data processing.

\textbf{User Interface Integration}:
One of the standout features of RAMPs is their ability to seamlessly integrate \gls{ui} components.
Unlike traditional plugins that might require significant adjustments to fit into an application's existing \gls{ui}, RAMPs come equipped with microfrontends.
These microfrontends are designed to be context-aware, ensuring that they align with the primary application's interface.
Furthermore, \glspl{ramp} introduce the possibility of multi-step \gls{ui} interactions, where users can be presented with sequential interfaces based on prior inputs or processing results, enhancing the depth and interactivity of the user experience.

\textbf{Data Processing Capabilities}:
Beyond the \gls{ui}, \glspl{ramp} are adept at handling intricate data processing tasks.
They encapsulate specific functionalities and offer them as self-contained services.
This modular approach to data processing ensures that \glspl{ramp} can be easily integrated, replaced, or updated without disrupting the overall system's functionality.
The multi-step nature of \glspl{ramp} also allows for iterative processing, where data can undergo multiple rounds of processing based on user inputs or intermediate results.

In essence, RAMPs represent a paradigm shift in how plugins are perceived and integrated within applications.
By merging \gls{ui} and data processing functionalities into a single, reusable unit and introducing multi-step interactions, they enhance the flexibility and adaptability of a system.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphics/qhanaarch.png}
  \caption{\gls{qhana}'s Architecture as depicted in \cite{Buehler2022}}
  \label{fig:qhanaarch}
\end{figure}

\section{Quantum Computing}
\label{sec:quantumComputing}
Quantum computing is a cutting-edge field that exploits the principles of quantum mechanics to process information.
Unlike classical computers that use bits (0s and 1s) to store and process information, quantum computers use quantum bits, or \emph{qubits}.
Through superposition and entanglement, qubits can exist in multiple states at once and be correlated with each other in ways that classical bits cannot \cite{Nielsen2010}.

Superposition allows a qubit to be in a state of a combination of 0 and 1, with a certain probability for each.
This property enables quantum computers to perform many calculations simultaneously, vastly increasing their potential computational power.
Entanglement allows qubits to be intimately linked regardless of the distance separating them.
A change in the state of one will instantaneously affect the state of the other, a phenomenon that Einstein famously called spooky action at a distance \cite{Einstein1935}.
This property is essential for many quantum algorithms, quantum error correction, and quantum teleportation, making it a fundamental resource in quantum information processing \cite{Nielsen2010, Preskill1998}.


\section{Variational Quantum Algorithms}
\label{sec:variationalQuantumAlgorithms}

\glspl{vqa} combine the principles of quantum computing and optimization uniquely and powerfully.
They are a class of hybrid quantum-classical algorithms that leverage the strengths of both quantum and classical computing to solve complex problems \cite{McClean2016}.

The central concept of \glspl{vqa} is to use a sequence of quantum operations (a \emph{quantum circuit}) controlled by specific parameters.
These parameters are adjusted using classical optimization techniques to solve a specific problem.
This problem, in many cases, involves finding the lowest energy state, or \emph{ground state}, of a quantum system.
This problem maps to finding the minimum of a particular function \cite{Peruzzo2013}.

By leveraging classical optimization algorithms, \glspl{vqa} become more resistant to quantum errors, as most of the computation is performed on a classical computer.
This combination of quantum and classical resources makes \glspl{vqa} a promising algorithm for near-term quantum devices \cite{Moll2017}.


\section{Optimization Algorithms}
\label{sec:optimizationAlgorithms}
Optimization is a powerful tool ubiquitous in various scientific and technological domains.
At its core, optimization is about finding the best solution from a set of possible choices.
This section provides a snapshot of optimization's fundamental principles, paving the way for deeper exploration in the context of \glspl{vqa} and plugin-based \glspl{vqa}.

\subsection{Objective Functions}
\label{subsec:objectiveFunctions}
\glspl{of} are fundamental to optimization problems, underpinning many computational algorithms and models.
Depending on specific requirements, the aim might be to minimize or maximize these functions.
Notably, within the realm of \glspl{vqa}, the focus is primarily on function minimization \cite{Weinan2017}.

The core inputs to a \gls{of} typically encompass data points (denoted as $x$), corresponding labels or outcomes (represented by $y$), and a set of parameters or weights (often symbolized by $\theta$ or $w$).
These parameters dictate how the model responds to the input data and makes predictions.
Additionally, certain \glspl{of} may also include hyperparameters as input, which control the behavior and complexity of the model.
In the context of optimization problems, the role of an \gls{of} is to capture both the problem we are attempting to solve and the strategy by which we are trying to solve it.
It provides a measure of the 'goodness' or 'fitness' of our current solution or parameters, and the aim is to adjust these parameters to improve this measure.

One example of an \gls{of} is the Lasso (Least Absolute Shrinkage and Selection Operator) Loss function.
The Lasso loss function is represented as:
\[
L(Y, X, W, \lambda) = ||Y - XW||^2_2 + \lambda ||W||_1
\]
In this equation:

\begin{itemize}
  \item \(Y\) is the vector of observed values.
  \item \(X\) is the matrix of input data points.
  \item \(W\) is the vector of weights, the model's parameters.
  \item \(\lambda\) is the regularization parameter, a non-negative hyperparameter.
\end{itemize}

This function consists of two terms:
\begin{enumerate}
  \item The first term \(||Y - XW||^2_2\) is the mean squared error between the predicted and actual outcomes.
  It measures the discrepancy between the model's predictions and the true values.
  \item The second term \(\lambda ||W||_1\) is a regularization term, where \(||W||_1\) represents the L1 norm (sum of absolute values) of the weight vector.
  This term penalizes the absolute size of the coefficients, encouraging them to be small.
\end{enumerate}
The hyperparameter \(\lambda\) governs the trade-off between these two terms.
When \(\lambda = 0\), the \gls{of} reduces to ordinary least squares regression, and the weights are chosen to minimize the mean squared error alone.
As \(\lambda\) increases, more weight is given to the regularization term, and the solution becomes more sparse (i.e., more weights are driven to zero).
Increasing the regularization term can help to prevent overfitting by effectively reducing the complexity of the model \cite{ShalevShwartz2014}

\subsection{Minimization Functions}
\label{subsec:minimizationFunctions}
Minimization functions, generally called optimization algorithms, are pivotal in many computational models and algorithms.
In essence, they serve to iteratively enhance the parameters of a model to reduce the value of the \gls{of}.
These minimization functions aim to find the optimal set of parameters that yield the lowest possible value of the \gls{of} within the constraints of the problem \cite{Nocedal2006}.

The process of optimization involves a search through the parameter space.
This search can be visualized as navigating a landscape of hills and valleys, with each point in the landscape corresponding to a different set of parameters and the height at each point representing the value of the \gls{of}.
The goal of the minimization function is to find the lowest point in this landscape, corresponding to the minimum value of the \gls{of} \cite{Goodfellow2017}.

The core inputs to a minimization function are the initial parameters of the model or weights (denoted as \(\theta\) or \(w\)),
the \gls{of} that needs to be minimized, and optionally the gradients of the \gls{of} with respect to the parameters.
Additionally, certain minimization functions may include hyperparameters as input, which control the behavior and complexity of the optimization process \cite{Virtanen2020}.
For instance, the learning rate is a typical hyperparameter that determines the step size in each iteration of the optimization process.

Numerous minimization functions are used in computational problems, each with its own strengths and weaknesses.
These functions range from simple methods, such as gradient descent, to more complex ones, such as Newton's method, stochastic gradient descent (SGD), RMSprop, or Adam.

One of the most fundamental and widely used minimization functions is the Gradient Descent.
To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the function's gradient (or approximate gradient) at the current point.

The update rule of gradient descent is given as follows:

\[
\theta_{t+1} = \theta_t - \alpha \nabla F(\theta_t)
\]

In this formula:

\begin{itemize}
  \item \(\theta_{t+1}\) represents the parameters at the next time step.
  \item \(\theta_t\) represents the current parameters.
  \item \(\alpha\) is the learning rate, a positive scalar determining the step size.
  \item \(\nabla F(\theta_t)\) is the gradient of the \gls{of}.
\end{itemize}

Here, the \gls{of} \(F\) is assumed to be a differentiable function.
The gradient \(\nabla F(\theta_t)\) provides the direction of the steepest ascent at the point \(\theta_t\), and \(-\nabla F(\theta_t)\) provides the direction of the steepest descent.
We move towards the minimum of the function by taking a step in this direction.

The steps' size is determined by the learning rate \(\alpha\), which is a hyperparameter that must be set before the learning process begins.
The learning rate controls how fast or slow we move toward the optimal weights.
We may skip the optimal solution if the learning rate is too large.
We may need too many iterations to converge to the best values if it is too small.

The choice of minimization function can significantly influence the efficiency and success of the optimization process.
While some minimization functions may perform well on certain problems, they may yield different results on others.
Therefore, it is crucial to understand the underlying mechanisms of these functions and their suitability to the specific problem at hand.

\
\chapter{Problem Statement and Objectives}
\label{chap:problem}

Optimization algorithms, with their ability to find the best possible solution from a set of feasible solutions, play a pivotal role in numerous computational domains.
\glspl{vqa} are a subset of these algorithms that leverage quantum computing principles, particularly in the realm of \glspl{of} and minimization techniques.
However, the true potential of optimization is often hindered by rigid platforms where the components of these algorithms are tightly integrated, limiting adaptability and innovation.

\gls{qhana}, with its unique environment tailored for experimenting with a myriad of machine learning and quantum algorithms, presents an opportunity to redefine this paradigm.
However, its current architecture does not fully exploit the modular benefits that can be achieved by decoupling the components of optimization algorithms.
Furthermore, while developing this modular framework, allowing plugins to interact with each other is essential.
This interaction-centric concept, once established, can be universally applied across \gls{qhana}, not just for optimization but for any scenario where plugin interaction is required.

\textbf{Problem Statement:}
How can we design and implement a modular framework within \gls{qhana} that allows components of optimization algorithms, specifically \glspl{of} and minimization functions, to be encapsulated as distinct, interchangeable plugins?
Furthermore, how can these plugins be structured to communicate and collaborate seamlessly, especially in the context of \glspl{vqa}?

This problem encompasses several challenges:

\begin{itemize}
    \item \textbf{Communication:} Establishing a robust communication mechanism that enables interaction, data sharing, and collaboration among these plugins.
    \item \textbf{Interchangeability:} Designing a system where different \gls{of} and minimization plugins can be effortlessly swapped, ensuring adaptability in optimization and \glspl{vqa}.
    \item \textbf{Standardization:} Implementing a consistent interface for these plugins, ensuring uniformity and compatibility across various \gls{of} and minimization plugins.
    \item \textbf{User Experience:} Providing an intuitive environment where users can easily select, interchange, and experiment with different optimization components tailored to their needs and provide developers with an extensible framework to build new minimization and \gls{of} plugins.
\end{itemize}

Addressing this problem is essential to enhance the capabilities of \gls{qhana}, transforming it into a dynamic, adaptable, and user-centric platform for optimization and VQAs.
The subsequent sections of this thesis will delve into the methodologies, implementations, and evaluations related to this problem.



\chapter{Methodology}
\label{chap:methodology}
\section{Conceptual Framework}
\label{sec:conceptualFramework}

This section outlines the theoretical and conceptual groundwork that anchors the methodology for this thesis.
The framework is rooted in modularity, interactivity and interchangeability principles in the context of plugin-based architectures for optimization algorithms in \gls{qhana}.

\subsection{Plugin-Based Architecture in QHana}
\gls{qhana}, with its design centered on the Digital Humanities, provides an extensible platform for experimenting with various algorithms.
\gls{qhana}'s architecture predominantly revolves around the concept of RAMPs.
The objective is to leverage \gls{qhana}'s inherent modularity by enabling components of optimization algorithms to function as distinct and interchangeable plugins.
This brings us to the significance of modularity in optimization.

\subsection{Significance of Modularity in Optimization}
When \glspl{vqa} are designed with modular components, it allows for increased flexibility.
Specifically, having distinct \glspl{of} and minimization functions means parts of the algorithm can be adjusted or replaced seamlessly.
This reflects the goals of modularity and flexibility intrinsic to \gls{qhana}'s design.

\subsection{Interactivity Between Plugins}
Interactivity in \gls{qhana} should encompass various facets:

A plugin should be able to invoke the microfrontend of another plugin.
Control should revert to the invoking plugin once the invoked plugin completes its tasks.
Both short-running tasks and long-running tasks should be facilitated.
For the latter, a callback mechanism is proposed, wherein the invoking plugin can be notified upon completing a long-running task by the invoked plugin.

The need for such interactivity stems from the inherent nature of optimization.
The \glspl{of} and minimization must closely interact to produce meaningful optimization results.
Moreover, since an invoking plugin is unaware of the parameters or requirements of the invoked plugin, direct interaction becomes imperative for dynamic data exchange and collaborative processing.

Drawing inspiration from existing systems, the interaction between the minimization and \glspl{of} in implementations like SciPy's \emph{optimizer.minimize} \cite{Virtanen2020} function serves as a precedent.

This thesis introduces the innovative concept of \emph{interaction endpoints} to elevate the degree of interactivity between plugins within \gls{qhana}.
While \gls{qhana} already employs a metadata field for plugins known as \emph{entry points} - endpoints invoked by the \gls{qhana} \gls{ui} to render the \gls{ui} of a plugin - interaction endpoints extend this paradigm by marking specific endpoints as callable by other plugins.

A defining characteristic of an interaction endpoint is its \emph{type}.
Interaction endpoints sharing the same type uphold uniformity in their signature and return type.
This typing ensures that they are invoked consistently by other plugins, irrespective of their specific implementation.

\subsection{Integration and Alignment with QHana}
The proposed approach complements \gls{qhana}'s existing principles, resonating with its emphasis on extensibility, adaptability, and user-centricity.
By enhancing the interactivity and modularity of plugins within \gls{qhana}, we strive to elevate its capabilities, making it a more dynamic platform for optimization and \glspl{vqa}.

\section{Architectural Design Strategy}
\label{sec:architecturalDesignStrategy}

The architectural design process is a critical phase that dictates how the solution's components will function and interact with each other.
The methodology undertaken for the architectural design of the plugin-based optimization framework consisted of a series of well-defined steps, ensuring clarity, modularity, and extensibility.

\subsection{Decomposition into Plugins}
The first step involved decomposing the optimization process into distinct plugins.
Decomposition is achieved by reviewing relevant literature to understand standard practices and methodologies in optimization \cite{Virtanen2020, Nocedal2006, ShalevShwartz2014, Weinan2017}.
The main challenge is to find a good decomposition so that, on the one hand, all functionalities that should be interchangeable are encompassed in an extra plugin,
on the other hand, splitting the process into too many plugins would create an overhead that would lead to inefficiency.

\subsection{Defining Plugin Responsibilities}
Once the plugins are identified, the next step is to define the responsibilities of each plugin precisely.
This ensured that every plugin had a well-defined purpose, preventing overlaps in functionality and ensuring clarity in their roles.
To delineate these responsibilities, several crucial decisions had to be made:

\begin{itemize}
  \item Which plugin is tasked with calculating the loss function?
  \item Which one handles the minimization process?
  \item Who prompts the user for the \gls{of} hyperparameters?
  \item Who gathers the minimization function hyperparameters from the user?
  \item Who inquires about the user's preference for the \gls{of} and minimization algorithm?
  \item Who solicits the input data from the user?
  \item Who requests the target variable?
  \item And lastly, who oversees and coordinates the entire process?
\end{itemize}

\subsection{Universal Plugin Interface Design}
Recognizing the vast possibilities and variations that interchangeable plugins might encompass, it is crucial to formulate a universally adhered-to interface.
This interface acted as a \emph{contract}, ensuring that irrespective of the specific implementation details of a plugin, there esists a consistent mode of interaction.
Core attributes and functionalities, like querying the number of initial weights for the minimization process required, are defined as part of this universal interface.
This approach fostered interchangeability and adaptability, as the defined interface could accommodate a multitude of interchangeable plugins, each with its unique functionalities.
In terms of optimization, this could mean that in the case of an \gls{of} plugin responsible for calculating the loss function, interfaces must be generalized to allow for any thinkable type of loss function.


\subsection{Plugin Interaction Design}
The next step is to design the interaction between plugins.
This design process involved defining the various ways in which plugins could interact with each other.
It is crucial to build a robust and flexible interaction design to accommodate many scenarios since the interaction between plugins in \gls{qhana} should be wider than the optimization process.
This interaction design should lay the foundation for all future plugin interactions within \gls{qhana}.

Scenarios included, for instance, a plugin invoking the microfrontend of another plugin, or it could call a specific endpoint of another plugin.
Additionally, the interaction design also encompasses the flow of control between plugins.
For instance, a plugin could invoke another plugin and wait for it to complete its tasks before resuming its own.
A plugin could also invoke another plugin and then stop execution while providing the invoked plugin with a callback URL that the invoked plugin could call once it completes its tasks.
Alternatively, a plugin could invoke another plugin and then continue with its tasks without waiting for the invoked plugin to complete its tasks.
Intrinsic to this step is the design of a coordination mechanism that facilitated the interaction between plugins.

\subsection{UML Diagrams}
To represent the responsibilities and interfaces of each plugin, component diagrams are created.
A diagram encapsulates the primary tasks of a plugin and the interfaces it provides, offering a clear understanding of the system's structure.

Sequence diagrams are devised to fathom the intricate interplay between plugins.
These diagrams capture the stepwise interaction between plugins.
They detail the control and data flow, spotlighting the sequence of plugin invocations and collaborative processes.


\subsection{Feedback and Refinement}
Throughout the design process, continuous feedback loops are integrated. This involved:
\begin{itemize}
\item Revisiting each stage, evaluating its alignment with overarching goals.
\item Making necessary refinements to ensure the architecture is robust and flexible.
\end{itemize}

With the architectural design strategy in place, the next step is to define the implementation strategy.

\section{Implementation Strategy}
\label{sec:implementationStrategy}

\subsection{Preliminary Analysis and Design}
Before diving into the implementation, an in-depth study of the \gls{qhana} documentation \cite{FabianBuehler} and a related paper on \gls{qhana}'s architecture \cite{Buehler2022} was undertaken to gain a comprehensive understanding of the system.
With component and sequence diagrams already in place, the next step is to define the implementation strategy.

\subsection{Development Environment and Toolset}

\begin{itemize}
    \item \textbf{Visual Studio Code}: A versatile integrated development environment employed for its adaptability and support for Python development, facilitating comprehensive coding, debugging, and testing for the project.

    \item \textbf{Docker}: Utilized to run all components of \gls{qhana}, ensuring consistent behavior across different computing environments.

    \item \textbf{Postman}: An API testing tool employed to validate and debug various endpoints, ensuring consistent and expected behavior of the plugin interactions.

    \item \textbf{Python}: As \gls{qhana} is implemented in Python offering versatility and a vast library ecosystem.

    \item \textbf{HTML}: Used for creating the microfrontends forming the user interfaces of the plugins.

    \item \textbf{Flask}: A lightweight Python web framework employed to develop the web services and \gls{rest}ful APIs for the plugins.

    \item \textbf{Marshmallow}: A Python library pivotal for object serialization/deserialization, ensuring structured data transfer between the plugins.

    \item \textbf{Flask-Smorest}: An extension of Flask, offering tools for building \gls{rest}ful APIs with Flask and Marshmallow, ensuring structured and accurate data transfer between plugins.

    \item \textbf{Celery}: Integrated to manage long-running tasks, particularly for minimizer plugins, allowing for asynchronous task execution.

    \item \textbf{Requests}: A Python library fundamental for facilitating interactions between plugins via HTTP requests.
\end{itemize}

\subsection{Key Principles of QHana Plugins}

In the \gls{qhana} ecosystem, plugins play a pivotal role in extending functionality.
The creation and integration of these plugins are governed by a set of principles that ensure their seamless operation and interaction.
For the context of this thesis, the following principles are deemed most significant:

\begin{itemize}
    \item \textbf{Plugin Definition:}
    A plugin is essentially a Python module or package.
    It contains a class that inherits from \texttt{QHAnaPluginBase} and should be located in a directory specified by the \texttt{PLUGIN\_FOLDERS} configuration variable.
    The plugin runner imports all plugins from these directories, handling only the root module of the plugin.
    The plugin, in turn, is responsible for importing its implementation class and all associated Celery tasks.

    \item \textbf{Plugin Metadata and Endpoints:}
    A plugin should contain metadata and links to all its endpoints. It is typically reachable via the "./" path.
    The metadata includes crucial information like entry points.

    \item \textbf{UI Interaction:}
    Plugins define both \texttt{href} and \texttt{hrefUi} to point to their microfrontend.
    The `hrefUi` serves the microfrontend where users input data and `href` processes this input.

    \item \textbf{Data Handling in Multi-step Plugins:}
    For plugins requiring multiple user input steps, data is stored in a key-value store with dictionary-like functionality.
    Data specific to a plugin task is associated with a unique database ID, and subsequent endpoint URLs of a plugin with this ID follow the pattern \texttt{http(s)://…/<UUID>/endpointName}.

    \item \textbf{Handling Long Running Tasks:}
    Long-running tasks utilize Celery. Task names, incorporating the plugin name, must be unique.

    \item \textbf{File Loading from URLs:}
    Plugins are designed to load files from URLs.

    \item \textbf{Data Format Specification:}
    Data formats, especially those shared across plugins, should be defined per the guidelines of \gls{qhana}.
    For instance, for the \texttt{text/csv} format pertaining to entities:
    \begin{itemize}
        \item The first column must be the ID column (named ID). Subsequent columns represent entity attributes.
        \item The CSV file should contain a header row specifying all attribute names.
    \end{itemize}
\end{itemize}

It is imperative to note that while the outlined principles are crucial to this thesis, \gls{qhana}'s overarching documentation provides a more exhaustive list of best practices and guidelines for plugin creation \cite{FabianBuehler}.


\subsection{Data Handling and Transfer}
Flask-Smorest is instrumental in ensuring structured and accurate data transfer between plugins.
It aided in validating the correctness of passed data, returning appropriate error codes, and managing errors efficiently.
This ensured that data exchanges between plugins are smooth and error-free, especially in the context of \glspl{vqa}.

\subsection{Testing and Debugging}

The quality and reliability of plugins and their interactions are paramount.
For this reason, a multifaceted testing strategy is adopted:

\begin{enumerate}
    \item \textbf{Static Code Analysis}:
    \begin{itemize}
        \item \textbf{Purpose}: To ensure code quality and maintainability and to identify potential vulnerabilities or deviations from coding standards.
        \item \textbf{Tools \& Implementation}: The tool \texttt{flake8} is utilized to conduct static code analysis on the Python codebase.
        By running \texttt{flake8}, a report detailing any code inconsistencies, potential errors, or areas for improvement is generated, providing valuable feedback for refinement.
    \end{itemize}

    \item \textbf{Logging and Monitoring}:
    \begin{itemize}
        \item \textbf{Purpose}: To capture, store, and analyze real-time information about the system's operations, aiding in troubleshooting and understanding system behavior.
        \item \textbf{Tools \& Implementation}: Python's in-built \texttt{logging} package is leveraged to track and record various events during the execution of plugins.
        By strategically placing logging statements within the code, it is possible to gain insights into the flow of operations, detect anomalies, and pinpoint areas that might require attention.
    \end{itemize}

    \item \textbf{Interactive Debugging}:
    \begin{itemize}
        \item \textbf{Purpose}: To step through the code in real-time, inspect variables, and analyze the program's flow to identify and rectify issues.
        \item \textbf{Tools \& Implementation}: The integrated Python debugger in Visual Studio Code is employed.
        This debugger allowed for setting breakpoints, stepping through code, inspecting variable states, and examining the call stack, providing a granular view of the system's operations and aiding in issue identification and resolution.
    \end{itemize}

    \item \textbf{Manual Testing}:
    \begin{itemize}
        \item \textbf{Purpose}: To capture nuances and potential issues that might be overlooked.
        \item \textbf{Procedure}: A hands-on approach is adopted where the plugins are interactively used.
        This involved navigating through the \gls{ui}, experimenting with different inputs, and observing the system's reactions to ensure it behaved as expected and met user requirements.
    \end{itemize}
\end{enumerate}

By employing a combination of static code analysis, detailed logging, interactive debugging and manual testing, it is ensured that the plugins are functionally correct and adhered to coding standards and best practices, guaranteeing a robust and user-friendly experience.


\subsection{Performance Optimization}
For optimal performance in the microservice-based plugin architecture, several strategies are employed:

\begin{enumerate}
    \item Reducing the number of interactions between plugins.
    \item For each interaction, the amount of data transmitted across the network should be kept to a minimum to ensure efficient communication and faster response times.
    \item For tasks that require extended processing, the Celery framework should be utilized, allowing these tasks to operate asynchronously and optimizing resource usage.
\end{enumerate}

These strategies are critical in ensuring swift and seamless interactions between plugins.


\subsection{Documentation and Extensibility}
Comprehensive documentation detailing how to add new \gls{of} and minimization plugins is created to foster growth and encourage future development.
This documentation serves as a guideline for developers aiming to extend the capabilities of \gls{qhana} with plugin interaction.


\section{Evaluation Strategy}
\label{sec:evaluationStrategy}

The implementation section of this thesis will ultimately present two distinct versions of a plugin-based optimization framework.
The evaluation will assess both versions to facilitate a comparison between the two approaches.
Additionally, the evaluation will juxtapose these approaches with a non-plugin-based approach, presented as a jupyter notebook.
To validate the effectiveness of the proposed solutions and to assess whether the goals set out in the problem statement have been achieved, the following evaluation methodologies are adopted:

\subsection{Performance Benchmarking}
\label{subsec:performanceBenchmarking}
Performance is paramount, especially in a plugin-based architecture.
A direct comparison will be made between the two plugin-based approaches and the non-service-based approach.
Critical metrics for this comparison include:

\begin{itemize}
\item \textbf{Objective Function Calculation Time}: This measures the time taken to compute the loss, directly impacting the overall optimization time.
For the service-based approach, the evaluation encompasses the calculation time of the \gls{of} and the time taken for a call to the \gls{of} calculation endpoint.

\item \textbf{Minimization Time}: This refers to the time consumed to minimize the \gls{of}.
In the service-based approach, the time taken for the minimization endpoint to return a result is also accounted for.

\item \textbf{Network Latency}: This quantifies the time required for a request to reach the server and for the corresponding response to be received.
This metric is exclusive to the service-based approach.

\item \textbf{Database Access Times}: It is essential to gauge the time required to retrieve data from the database since endpoints access context data during each invocation.
\end{itemize}

This evaluation hinges on quantitative metrics, with results graphically depicted for enhanced clarity.
The \emph{time.perf\_count} function from Python measures the time taken for a function to execute, providing a reliable and accurate measure of performance.

\subsection{Interchangeability}
The accurate measure of interchangeability is the ability to swap components without causing disruptions.
Accordingly, every plugin of a particular type is tested against every other plugin of the same type to validate seamless interchangeability.

\subsection{Standardization Adherence}
Standardization is tested to ensure compatibility and uniformity across diverse plugins.
An evaluation determines whether all implemented plugins conform to the prescribed standards.
This guarantees that all optimization plugins can be implemented uniformly, facilitating consistent and compatible integrations in the future.

\subsection{User Experience}
While the primary audience for this thesis is developers for the \gls{qhana} platform, the user experience has to be maintained.
An assessment determines the ease and efficiency with which a developer can introduce a new plugin in plugin-based optimization in \gls{qhana}.
The complexity and steps of to integrate a new, interchangeable plugin for the optimization process are laid out.

Through meticulously evaluating these parameters, this thesis thoroughly appraises the solutions concerning the challenges outlined in the problem statement.

\section{Test Data Generation for Evaluation}
To robustly evaluate the solutions proposed in this work, testing them on diverse datasets, reflecting both size and complexity is essential.
The objective is to mimic real-world scenarios where optimization problems can range from simple tasks with a few data points to complex challenges with many features and data points.
The code used to generate the test data can be found in the appendix \ref{chap:appendix}.

\subsection{Dataset Characteristics:}

\begin{enumerate}
    \item \textbf{Variability in Size}: Datasets of different sizes have been generated, spanning from a modest 10 data points to a substantial 100,000 data points.
    This variation ensures that the optimization performance is assessed across different scales, from quick-to-process small datasets to computationally demanding large datasets.

    \item \textbf{Features Determined by Size}: The number of features, \( x \), in each dataset, is dynamically determined based on the dataset's size, calculated explicitly as \( \text{int}(\sqrt{\text{size}} \times 1.5) \).
    This approach ensures that with the growth of the dataset, its complexity also increases, mirroring real-world scenarios where larger datasets often present more features or dimensions.

    \item \textbf{Synthetic Data with Noise}: The data is synthetically generated using the \texttt{make\_regression} function from the Scikit-learn library, known for its utility in machine learning.
    An added noise parameter introduces an element of randomness, making the optimization task more intricate and resembling real-world challenges.

    \item \textbf{Ensuring Reproducibility}: To maintain the consistency and reliability of the generated datasets across multiple runs or evaluations, a fixed random seed (`np.random.seed(42)`) has been set.
    This ensures that the data, though synthetic and noisy, remains consistent across evaluations, enabling accurate comparisons, assessments, and reproducibility.

    \item \textbf{Adherence to QHana Standards}:
   \begin{itemize}
        \item \textbf{Unique Entity IDs}: Each data point in the dataset is allocated a unique ID in the format \emph{entityX}, where X represents the entity number.
This aligns with \gls{qhana}'s data standard that mandates every data point to have an identifiable ID.
        \item \textbf{Data Format and Storage}: All datasets are stored in the CSV (Comma Separated Values) format, adhering to \gls{qhana}'s accepted data formats.
    \end{itemize}
\end{enumerate}

By employing such multifaceted and representative datasets, the methodology aims to objectively evaluate the optimization solutions, assessing their performance, interchangeability, and user experience across various scenarios.

\chapter{Resulting System Architecture}
\label{chap:architecture}


\section{Resulting Decomposition into Plugins and their Responsibilities}
\label{sec:resdecomposition}

Following the decomposition strategy, the plugin-based optimization framework is divided into three primary plugin types:
\gls{of} plugin, the minimizer plugin, and the coordinator plugin.
These plugins have specific roles and responsibilities, ensuring a modular and efficient optimization process.
This split is visualized in the component diagram in figure \ref{fig:component_diagram}.
It is important to note that this decomposition and the associated responsibilities remain consistent for both proposed plugin-based approaches.

\subsection{Objective Function Plugin}

The \gls{of} plugin is central to the optimization framework, encapsulating the mathematical function that defines the problem at hand.
Its primary roles include:

\begin{itemize}
\item \textbf{Metadata Provision}: The plugin offers metadata about itself.
\item \textbf{Hyperparameters Acquisition}: It prompts the user to provide the hyperparameters necessary for the loss function calculation.
\item \textbf{Loss Calculation}: Based on the provided input data, the plugin computes the loss, representing the discrepancy between predicted and target values.
\item \textbf{Gradient Calculation (Optional)}: For optimization algorithms that leverage gradient-based methods, the plugin can optionally compute the gradient of the loss function.
This aids in guiding the optimization process towards the desired minimum.
\end{itemize}

\subsection{Minimizer Plugin}

The minimizer plugin is responsible for iteratively adjusting parameters to minimize the loss provided by the \gls{of} plugin.
Its functions include:

\begin{itemize}
\item \textbf{Metadata Provision}: Similar to the \gls{of} plugin, it provides essential metadata.
\item \textbf{Hyperparameters Acquisition}: The plugin acquires the hyperparameters crucial for the employed minimization algorithm from the user.
\item \textbf{Minimization Process}: Using the loss (and optionally the gradient) from the \gls{of} plugin, the minimizer plugin endeavors to find the parameter values that minimize this loss.
\end{itemize}

\subsection{Coordinator Plugin}

The Coordinator plugin acts as the orchestrator, ensuring seamless interaction between the \gls{of} and Minimization plugins and the user.
Its primary responsibilities are:

\begin{itemize}
\item \textbf{Plugin Selection}: It prompts the user to select the desired \gls{of} and minimizer plugins for optimization.
\item \textbf{Data Acquisition}: The plugin gathers the necessary input data and the target variable from the user, which will be used in the optimization process.
\item \textbf{Endpoint Acquisition}: It obtains the necessary endpoints from the selected plugins, which will be used for interaction.
\item \textbf{Coordination Role}: It manages the interaction between the \gls{of} and minimizer plugins, ensuring that the loss (and optionally gradient) calculation function is provided to the minimizer plugin for the optimization process.
Additionally, it coordinates the interaction between the user and the selected plugins, ensuring the user is provided with the necessary microfrontends.
\item \textbf{Results Presentation}: Post-optimization, the coordinator plugin presents the optimization results to the user.
\end{itemize}


\section{Universal Plugin Interface Design}

The process of optimization is inherently complex, with a multitude of variations and nuances.
It is imperative to establish universal plugin interfaces for each type of plugin to ensure a streamlined interaction between different plugins.
This interface acts as a standard that every plugin of a specific type has to adhere to.
The interfaces for each plugin are detailed below and are visualized in the component diagram in figure \ref{fig:component_diagram}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{graphics/plugin_decomposition.svg}
    \caption{Component Diagram of decomposed optimization process}
    \label{fig:component_diagram}
\end{figure}

\paragraph{Objective Function Plugin Interfaces:}

The \gls{of} plugin interface is designed to accommodate a wide range of loss functions, including those that offer gradient computation.
Its interfaces are:

\begin{itemize}
\item \textbf{Metadata}: The plugin provides metadata about itself as specified in the \gls{qhana} documentation \cite{FabianBuehler}.
\item \textbf{UIRef}: This endpoint is used to obtain the microfrontend for the \gls{of} plugin, which is used to collect the hyperparameters from the user.
This interface also follows the \gls{qhana} documentation \cite{FabianBuehler}.
\item \textbf{HRef}: This endpoint is used to process the input from the \gls{of} microfrontend and is therefore usually called the \textbf{processing} endpoint.
It is usually triggered by the user clicking the \emph{submit} button on the microfrontend.
This interface also follows the \gls{qhana} documentation \cite{FabianBuehler}.
\item \textbf{PassData}: This endpoint is used to pass the input and target data to the \gls{of} plugin.
It returns the number of weights that are required for the optimization process.
The number of weights is returned here since it depends on the data and possibly the hyperparameters.
\item \textbf{CalculateLoss}: This endpoint calculates the loss.
\item \textbf{CalculateGradient (Optional)}: This endpoint is used to calculate the gradient of the loss function. It is optional since the gradient cannot be computed efficiently for all \glspl{of}.
\item \textbf{CalculateLossAndGradient (Optional)}: This endpoint calculates the loss and the gradient of the loss function. Similar to the previous endpoint, it is optional.
\end{itemize}

\paragraph{Minimizer Plugin Interfaces:}

The minimizer plugin, responsible for optimizing the loss function provided by the \gls{of} plugin, offers these interfaces:

\begin{itemize}
  \item \textbf{Metadata}: same as for the \gls{of} plugin
  \item \textbf{UIRef}: same as for the \gls{of} plugin
  \item \textbf{HRef}: same as for the \gls{of} plugin
  \item \textbf{Minimize}: This endpoint minimizes the loss function.
\end{itemize}

\paragraph{Coordinator Plugin Interfaces:}

The coordinator plugin, orchestrating the interaction between the \gls{of} and minimizer plugins, is equipped with the standard \gls{qhana} interfaces \textbf{Metadata}, \textbf{UIRef}, and \textbf{HRef}.

A systematic, consistent, and efficient optimization process is ensured by establishing these interfaces and ensuring that each plugin conforms to them.
This structured approach facilitates seamless interactions and fosters interchangeability, modularity, and extensibility, making it easy to add new \gls{of} and minimizer plugins in the future.

\section{Plugin Interaction Design}

The design of plugin interactions is pivotal to ensuring efficient and seamless coordination between different system components.
Given the extensive possibilities of interactions within the \gls{qhana} environment, the design phase is meticulous, considering various scenarios and ensuring adaptability.
The two primary modes of interactions are short-running and long-running, each catering to specific requirements.

\paragraph{Short-Running Interaction:}

In short-running interactions, a plugin invokes another plugin's endpoint, typically via a `GET` or a `POST` request, and immediately receives a response.
This mode of interaction is synchronous, wherein the invoking plugin waits for the response before proceeding.
Instances of such interactions in the optimization context are:
\begin{itemize}
    \item The coordinator plugin retrieves metadata from the \gls{of} and minimizer plugins.
    \item The coordinator plugin passes data to the \gls{of} plugin.
    \item The minimizer plugin calls the \texttt{CalculateLoss} endpoint of the \gls{of} plugin.
\end{itemize}

\paragraph{Long-Running Interaction:}

Long-running interactions come into play for processes that demand extensive computation time or involve multiple steps.
In such a case, the calling plugin invokes the endpoint of the invocable plugin.
The invoked endpoint then schedules a long-running task and returns immediately.
The calling plugin can then continue with its execution in an asynchronous manner without waiting for the long-running task to finish.
A new endpoint for the calling plugin is defined that can be called by the long-running task once it is finished.
This endpoint is called a callback endpoint.
Instances of such interactions are:
\begin{itemize}
    \item The coordinator schedules the microfrontend of the \gls{of} plugin.
    The coordinator plugin provides a callback endpoint to the \gls{of} plugin that is called once the user enters the input and the input is processed.
    \item The coordinator schedules the microfrontend of the minimizer plugin.
    This mechanism works identically to the previous item.
    \item The coordinator plugin calls the minimization endpoint of the minimizer plugin.
    The minimizer plugin schedules the minimization process and returns immediately.
    Once the minimization process is finished, the minimizer plugin calls a callback endpoint of the coordinator plugin.
\end{itemize}
The design of these interactions ensures that plugins can interact seamlessly and efficiently regardless of the complexity or duration of tasks.

\section{Introduction of Interaction Endpoints}
\label{sec:introie}

Building upon the idea of plugins interacting with each other, as detailed in the previous section, a crucial question arises:
How does one plugin discover the available endpoints of another?
This thesis introduces a novel concept called \textit{interaction endpoints} to \gls{qhana} to address this very challenge.

While \gls{qhana} already has a metadata field named \emph{entry points}, which are endpoints invoked by the \gls{qhana} \gls{ui} to render a plugin's \gls{ui}, interaction endpoints extend this idea further.
They specifically define endpoints in the metadata that other plugins can invoke, facilitating seamless integration and interaction.

The core of interaction endpoints is their \emph{type}.
All interaction endpoints with the same type must adhere to the same signature and return type.
This uniformity ensures that other plugins can interchangeably invoke them.
The \gls{of} plugin provides interaction endpoints of types \emph{calc\_loss}, \emph{calc\_grad}, \emph{calc\_loss\_and\_grad}, and \emph{of\_pass\_data}.
The minimizer plugin offers the `minimization' type.
These interaction endpoints correspond to the endpoints defined in the previous section.
The introduction of interaction endpoints significantly enhances the modularity and interchangeability within \gls{qhana}, paving the way for a more dynamic and adaptable plugin ecosystem.
More on how these interaction endpoints are implemented can be found in the implementation chapter \ref{chap:implementation}.

\section{Final Interaction Flow}
The architecture's final interaction flow can be broken down into three main parts, each ending in a new \gls{ui} displayed to the user.
They are visualized in the sequence diagrams in figures \ref{fig:interaction_flow_part1}, \ref{fig:interaction_flow_part2}, and \ref{fig:interaction_flow_part3}.

The first part begins after selecting the optimization plugin and concludes when the \gls{of} plugin's \gls{ui} is set as the next step.
Here, the coordinator plugin retrieves the user-selected \gls{of} and minimization plugin metadata, including their interaction endpoints.
This flow is represented in Figure \ref{fig:interaction_flow_part1}.

The second part starts with retrieving the \gls{of} plugin's microfrontend and ends when the minimizer plugin \gls{ui} is set as the next step.
After the user inputs hyperparameters and submits, the \gls{of} processes the input and sends a callback to the coordinator plugin,
The callback endpoint then passes the input and target data to the PassData endpoint.
This is depicted in Figure \ref{fig:interaction_flow_part2}.

The last segment starts with the minimizer plugin's microfrontend retrieval and ends when the optimization process concludes.
After users input the minimization hyperparameters, the minimizer processes the data and sends a callback to the coordinator.
The coordinator then triggers the minimization endpoint, initiating a long-running minimization task that continuously calls the \gls{of} calculation endpoint.
Once this task is completed, a final callback is made to the coordinator with the results.
This flow is shown in Figure \ref{fig:interaction_flow_part3}.

\begin{figure}[p]
  \centering
  \checkoddpage
  \ifoddpage
      \rotatebox{-90}{\includegraphics[width=0.9\textheight]{graphics/interaction_flow_1.svg}}
  \else
      \rotatebox{90}{\includegraphics[width=0.9\textheight]{graphics/interaction_flow_1.svg}}
  \fi
  \caption{Final interaction flow of the first part of the optimization process.}
  \label{fig:interaction_flow_part1}
\end{figure}

\begin{figure}[p]
  \centering
  \checkoddpage
  \ifoddpage
      \rotatebox{-90}{\includegraphics[width=0.9\textheight]{graphics/interaction_flow_2.svg}}
  \else
      \rotatebox{90}{\includegraphics[width=0.9\textheight]{graphics/interaction_flow_2.svg}}
  \fi
  \caption{Final interaction flow of the second part of the optimization process.}
  \label{fig:interaction_flow_part2}
\end{figure}

\begin{figure}[p]
  \centering
  \checkoddpage
  \ifoddpage
      \rotatebox{-90}{\includegraphics[width=0.9\textheight]{graphics/interaction_flow_3.svg}}
  \else
      \rotatebox{90}{\includegraphics[width=0.9\textheight]{graphics/interaction_flow_3.svg}}
  \fi
  \caption{Final interaction flow of the third part of the optimization process.}
  \label{fig:interaction_flow_part3}
\end{figure}




\chapter{Implementation}
\label{chap:implementation}

The reader can find the resulting code that realized two different approaches to a plugin-based optimization framework in \gls{qhana} 's GitHub repository\footnote{\url{https://github.com/UST-QuAntiL/qhana-plugin-runner}}.
As examples, the code implements two types of minimizer plugins, the \emph{scipy-minimizer} and \emph{scipy-minimizer-grad} plugins, and three types of \gls{of} plugins, the \emph{ridge-loss}, \emph{hinge-loss}, and \emph{neural-network} plugins.
More on their implementation can be found in the following sections.
This chapter does not aim to provide a comprehensive overview of the code but to highlight the critical implementation details and differences between the two approaches.

\section{Directory Structure and Plugin Loading}
\label{sec:directoryStructure}

\gls{qhana} maintains two primary directories for plugin management: the \textit{plugins} and the \textit{stable plugins} directories.
The former contains plugins undergoing development, while the latter contains deployment-ready plugins.
Within these directories, individual plugins are neatly organized into dedicated subfolders.
Upon initiation, \gls{qhana} scans and loads plugins from these designated subfolders.

The focus of this thesis, the optimization plugin, resides in the \textit{plugins/optimization} directory.
Further divisions are made to promote clarity and a structured layout.

\begin{itemize}
  \item \textit{plugins/optimization/coordinator} – For the coordinator plugin.
  \item \textit{plugins/optimization/objective\_functions} – Where each \gls{of} plugin occupies its respective subfolder.
  \item \textit{plugins/optimization/minimizer} – Where each minimizer plugin occupies its respective subfolder.
\end{itemize}

Given that \gls{qhana}'s native architecture does not support the direct loading of plugins from nested subdirectories, a recursive plugin loader is developed for this purpose.
This loader traverses through the \textit{plugins} directory and its subdirectories.
The presence of an \textit{\_\_init\_\_.py} file within a folder confirms the plugin's legitimacy.
To exclude a plugin from the loading process, one places a \textit{.ignore} file in its respective folder.
The maximum recursion depth is limited to four to maintain system performance and a clear, structured layout.
This threshold sufficiently accommodates the current plugin structure but can be adjusted upwards if future needs arise.

The folder hierarchy is further enriched by including an \textit{interaction\_utils} directory dedicated to housing utility functions for generalized plugin interactions.
Additionally, there is a \textit{shared} directory, which stores data structures and schemas utilized across the plugins related to the optimization plugin.
A visual representation of the final folder structure, with all plugins implemented for this thesis, is shown in \cref{fig:folderStructure}.


\begin{figure}[h!]
  \dirtree{%
      .1 plugins.
      .2 optimizer.
      .3 coordinator.
      .3 interaction\_utils.
      .3 minimizer.
      .4 scipy\_minimizer.
      .4 scipy\_minimizer\_grad.
      .3 objective\_functions.
      .4 hinge\_loss.
      .4 neural\_network.
      .4 ridge\_loss.
      .3 shared.
  }
  \caption{QHana Plugin Folder Structure.}
  \label{fig:folderStructure}
\end{figure}

\section{Maintaining Context Across Plugin Endpoints}
\label{sec:contextBetweenPluginEndpoints}

The implementation leverages the multi-step plugin concept inherent to \gls{qhana} to preserve the continuity of information across different plugin endpoints.
In the \emph{processing} endpoint of a plugin, a database task is established, identifiable by a unique database ID.
This task stores all relevant context data, such as the hyperparameters designated for the minimization process.
Subsequent endpoints, like the \emph{minimization} endpoint, can access this stored context data by referencing the database task.
Notably, the endpoints' URL incorporates the database task's ID.


\section{UI Creation for Plugin Interaction}
\label{sec:uiCreationForPluginInteraction}
The QHAna platform provides a streamlined mechanism to craft simple \glspl{ui} with input fields generated from marshmallow schemas.
The primary step is to inherit from QHAna's \texttt{FrontendFormBaseSchema} class.
For the \gls{of} plugin, the input fields are the hyperparameters the loss function needs.
Listing \ref{lst:ridge_loss_schema} shows the \emph{ridge\-loss} plugin's hyperparameter, which is the regularization parameter \emph{alpha}.

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption={Ridge Loss Plugin Hyperparameter Schema.}, label=lst:ridge_loss_schema]{code/hyperparameter_schema_ridge.py}
\end{minipage}

The minimizer and \gls{of} plugin selection field is a critical input field for the coordinator plugin.
The \texttt{PluginUrl} class,  a custom class that extends marshmallow's \texttt{fields.Url} class generates that field.
It ensures that only plugins of the correct type are selected by checking a plugin's metadata for the correct label.
Imperative for this to work is that all minimizer plugins have the \emph{minimization} and all \gls{of} plugins have the \emph{objective-function} label.
Listing \ref{lst:coordinator_schema} shows the minimizer plugin selection field in the coordinator plugin's \gls{ui}.

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=Coordinator Plugin UI Schema., label=lst:coordinator_schema]{code/ui_schema_coordinator.py}
\end{minipage}

As the \glspl{ui} of the \gls{of} and minimizer plugins are designed to be invoked by the coordinator plugin, the \emph{UIHref} which serves the \gls{ui} needs to accept a callback URL.
Therefore, all \emph{UIHref} endpoints of invocable plugins must include the \emph{CallbackUrlSchema} marshmallow schema as a query parameter.



\section{Invocation of Plugin Microfrontends by the Coordinator Plugin}
\label{sec:invocationOfPluginMicrofrontendsByTheCoordinatorPlugin}

As highlighted in the previous chapter, the coordinator plugin invokes the microfrontends of both the \gls{of} and minimizer plugins.
An adaptation of \gls{qhana}'s \emph{add\_step} function allows the invocation of a plugin's microfrontend by another plugin.
The \emph{add\_step} function's original purpose is to allow a multi-step plugin to show another of its microfrontends to the user.
The function is enhanced and renamed to \emph{invoke\_task}.
With this modification, the coordinator plugin can invoke another plugin's microfrontend by supplying the appropriate \emph{UIHref} and \emph{Href}.
Furthermore, the coordinator plugin can provide a callback URL to the invoked plugin.
This callback URL is a query parameter within the \emph{Href} and \emph{UIHref} URLs.

\section{Callback to the Coordinator Plugin}
\label{sec:implementationOfCallbacksToTheCoordinatorPlugin}

The coordinator plugin requires callbacks from invocable plugins in two distinct scenarios:
\begin{enumerate}
    \item After the user submits the microfrontend of either the minimization or the \gls{of} plugin.
    \item Once the asynchronous minimization process concludes.
\end{enumerate}

For the first scenario, the process unfolds as follows:
\begin{itemize}
    \item The coordinator invokes the microfrontend of the invocable plugin using the \emph{invoke\_task} function, as elaborated in Section \ref{sec:invocationOfPluginMicrofrontendsByTheCoordinatorPlugin}.
    \item On user submission of the microfrontend, the callback URL is passed to the processing endpoint of the invocable plugin via a query parameter.
    \item Subsequently, the invoked plugin makes a post request to the callback URL.
\end{itemize}

In contrast, the second scenario operates through the following mechanism:
\begin{itemize}
    \item The coordinator plugin initiates a post request to the minimizer plugin's minimization endpoint, embedding a callback URL within the body.
    \item The minimizer plugin schedules an asynchronous celery task for loss function minimization.
    It registers the callback URL in the database under the designation \emph{status\_changed\_callback\_urls}.
    This label is pivotal for subsequent retrieval of the callback URL.
    \item The endpoint returns, and the celery task commences the minimization process.
    \item Following the task's completion, its status updates through the already implemented \emph{save\_task\_result} function.
    This action now triggers a signal when it finishes or runs into an error, courtesy of Python's \emph{blinker} library, indicating the status change and passing the database ID of the task as an argument.
    \item A dedicated signal handler retrieves the callback and task view URL from the database.
    The handler then orchestrates a post request to the callback URL, embedding the task view URL within the body.
\end{itemize}
Listing \ref{lst:signal_handler} presents the implementation of the signal handler, while Listing \ref{lst:singal_emitter} illustrates the juncture where the signal is emitted.

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=Signal Handler for Callbacks., label=lst:signal_handler]{code/signal_handler.py}
\end{minipage}

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=Signal Emitter for Callbacks., label=lst:singal_emitter]{code/signal_emitter.py}
\end{minipage}

\section{Implementation of Interaction Endpoints}
\label{sec:implementationOfInteractionEndpoints}

The essence of the plugin-based optimization framework lies in its interaction endpoints.
These endpoints standardize the interchangeability of inputs and outputs.
Marshmallow schemas are the backbone of the implementation to manage this standardized data exchange.

\paragraph{Interaction Endpoint Schemas:}
The repository's shared folder contains the defined schemas.
Each interaction endpoint type has an associated input and output schema.
If a plugin wishes to invoke another plugin's interaction endpoint, it can populate the input schema and parse the response using the output schema.
In the implementation, the \gls{of} plugin defines schemas for the following interaction endpoints:
\begin{itemize}
  \item \textbf{pass\_data}: Uses \emph{ObjectiveFunctionPassDataSchema} for input and \emph{ObjectiveFunctionPassDataResponseSchema} for output.
  \item \textbf{calc\_loss}: Uses \emph{CalcLossOrGradInputSchema} for input and \emph{LossResponseSchema} for output.
\end{itemize}
For the minimizers' \emph{minimization} endpoint, the input schema is \emph{MinimizerInputSchema}.
The output, meanwhile, is a simple 200 status code, signaling the initiation of the minimization process.
Another implementation challenge revolves around ensuring that plugins can discover the endpoints of other plugins, which is solved in section \ref{sec:metadataEndpoint}, but first section
\ref{sec:customMarshmallowFieldForData} introduces a custom marshmallow field for input and target data.

\paragraph{Custom Marshmallow Field for Data:}
\label{sec:customMarshmallowFieldForData}
To streamline the input and target data exchange between plugins, a custom marshmallow field, \emph{NumpyArray}, is introduced (see \ref{lst:numpy_array_schema}).
Specifically crafted to handle NumPy arrays, this versatile field can accommodate arrays of any dimensionality with JSON-serializable data types.
It aids in data transfer between endpoints and ensures efficient data storage in the database.

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=Custom Marshmallow Field for NumPy Arrays., label=lst:numpy_array_schema]{code/numpy_array_schema.py}
\end{minipage}

\paragraph{Metadata Endpoint and Interaction Endpoint Discovery:}
\label{sec:metadataEndpoint}
The metadata of plugins houses the definitions for interaction endpoints.
Traditionally, the \emph{EntryPoint} field in the metadata is reserved for guiding the \gls{qhana} \gls{ui} to a plugin's microfrontend.
However, this thesis expands the \emph{EntryPoint} field to encompass interaction endpoints.
The \emph{interactionEndpoints} field is essentially a list, potentially containing multiple interaction endpoints.
Each list entry is a dictionary containing a \textbf{type} and an \textbf{href} field.
The \textbf{type} field, a string, denotes the endpoint type.
It is made user-friendly by providing the \emph{InteractionEndpointType} enum in the shared folder, which lists all possible endpoint types for the optimization process.
The \textbf{href} field houses the endpoint's URL.
Listing \ref{lst:metadata} illustrates this using the \emph{ridge-loss} plugin's metadata.

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=Metadata of the Ridge Loss Plugin., label=lst:metadata]{code/metadata.py}
\end{minipage}

For multi-step plugins, the interaction endpoint's definition in metadata slightly deviates.
As discussed in \ref{sec:contextBetweenPluginEndpoints}, multi-step plugins incorporate a runtime-specific database ID into the URL.
The \emph{href} field functions as a format string that allows the substitution of a placeholder with the actual database ID.
Listing \ref{lst:metadata_multistep} offers a glimpse into this, showcasing the \emph{ridge-loss} plugin implemented as a multi-step entity.
The invoking plugin replaces the placeholder with the actual database ID when known.
This distinction is foundational to the differences between the two approaches, explored further in the subsequent section.

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=Metadata of the Ridge Loss Plugin as a Multi-Step Plugin., label=lst:metadata_multistep]{code/metadata_multistep.py}
\end{minipage}

\section{Exemplary Minimizers and Objective Functions}
\label{sec:exampleMinimizersAndObjectiveFunctions}

The implementation encompasses two distinct minimizer plugins and three \gls{of} plugins to illustrate the capabilities of the plugin-based optimization framework.
Specifically, the minimizer plugins \emph{scipy-minimizer} and \emph{scipy-minimizer-grad} and the \gls{of} plugins \emph{ridge-loss}, \emph{hinge-loss}, and \emph{neural-network}.

\paragraph{Minimizer Plugins:}
\label{sec:minimizerPlugins}

At the core, both plugins use \emph{scipy.optimize.minimize} as their minimization algorithm.
The \emph{scipy-minimizer} plugin leverages this method to minimize the loss function without needing a gradient.
Its sole input hyperparameter is the minimization method, restricted to methods that operate without a gradient.

On the other hand, the \emph{scipy-minimizer-grad} plugin employs the same method but incorporates the gradient for loss minimization.
It accepts input hyperparameters that exclusively pertain to methods necessitating the gradient.
Regardless of their approach, both plugins yield the weights that optimize the loss function's minimization.

\paragraph{Objective Function Plugins:}
\label{sec:objectiveFunctionPlugins}

The \emph{ridge-loss} plugin encapsulates the ridge loss function, as depicted in \ref{alg:ridge_loss}.
It operates with the regularization parameter \emph{alpha} as its input hyperparameter.
In a similar vein, the \emph{hinge-loss} plugin embodies the hinge loss function, showcased in \ref{alg:hinge_loss}, and uses the regularization parameter \emph{C} as its input.

The \emph{neural-network} plugin employs a neural network featuring a single hidden layer.
Its hyperparameters include the count of neurons present in this hidden layer.
Notably, this plugin facilitates the gradient calculation for the loss function, making it compatible with the \emph{scipy-minimizer-grad} plugin.
The user is highly encouraged to refer to the code repository for a more in-depth understanding of how a more sophisticated method is implemented for loss calculation.


\begin{lstlisting}[caption={Ridge Loss Calculation}, label=alg:ridge_loss]
  def ridge_loss(X, w, y, alpha):
      # Calculate the predicted values using the weight vector
      y_pred = X.dot(w)

      # Compute the mean squared error between actual and predicted values
      mse = mean((y - y_pred) ** 2)

      # Calculate the ridge penalty using the weight vector
      ridge_penalty = alpha * sum(w**2)

      # Combine the mean squared error and ridge penalty to get the total ridge loss
      total_loss = mse + ridge_penalty

      return total_loss
\end{lstlisting}

\begin{lstlisting}[caption={Hinge Loss Calculation}, label=alg:hinge_loss]
  def hinge_loss(X, w, y, C):
      # Get the number of samples from the input matrix
      n_samples, _ = X.shape

      # Initialize the loss to zero
      loss = 0.0

      # Iterate over each sample in the dataset
      for i in range(n_samples):
          # Calculate the score for the current sample using the weight vector
          score = np.dot(w, X[i])

          # Compute the hinge loss for the current sample and accumulate
          loss += max(0, 1 - y[i] * score)

      # Apply the regularization term to the accumulated loss
      loss = C * loss / n_samples

      # Add the l2 regularization term to the loss
      loss += 0.5 * np.dot(w, w)

      return loss
\end{lstlisting}

\section{Comparative Analysis of Plugin-Based Implementation Strategies}
\label{sec:differencesBetweenTheTwoPluginBasedImplementationApproaches}

The implementation resulted in two distinct methodologies for constructing a plugin-based optimization framework.
This section outlines the intrinsic differences that characterize each strategy.

\subsection{Decoupled Plugin Approach}
\label{sec:firstApproach}

The foundational idea behind the first approach is promoting maximal decoupling between plugins, which particularly applies to the \gls{of} plugin.
This idea entails minimizing data transfer between plugins and emphasizing data retrieval directly from the database whenever feasible.

Specifically, the \gls{of} plugin archives its hyperparameters in the database.
When the \emph{pass\_data} endpoint is invoked, input and target data get stored in the database.
Thus, for the \emph{pass\_data} endpoint, which determines the number of input weights, the coordinator plugin only passes the input and target data since the \gls{of} plugin fetches the hyperparameters from the database.
The loss calculation endpoints \emph{calc\_loss}, \emph{calc\_grad}, and \emph{calc\_loss\_and\_grad} solely need the weights as input, leveraging already stored hyperparameters and data.
Listing \ref{lst:calc_loss_schema} illustrates an exemplary schema for the \emph{calc\_loss} endpoint.

Consequently, for the minimizer plugin, there is no occasion where knowledge of the \gls{of} 's hyperparameters or its data is essential, leading to a lean input schema for the \emph{minimization} endpoint, as depicted in Listing \ref{lst:minimization_schema}.
However, it mandates that the task's database ID be communicated to the coordinator plugin through the microfrontend callback process, essential for managing interaction endpoint URLs, as discussed in \ref{sec:metadataEndpoint}.

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=\emph{CalcLossOrGradInputSchema} Schema for multistep-plugin approach., label=lst:calc_loss_schema]{code/calc_loss_schema.py}
\end{minipage}

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=\emph{MinimizerInputSchema} Schema for multistep-plugin approach., label=lst:minimization_schema]{code/minimization_schema.py}
\end{minipage}

\subsection{Integrated Plugin Approach}
\label{sec:secondApproach}

The second strategy embodies a tighter integration.
This approach does not implement the \gls{of} plugin as a multi-step plugin.
Such an implementation implies that no shared context exists among the plugin's endpoints, necessitating the passing of all required data during each endpoint invocation.

This requires the microfrontend callback process to relay the hyperparameters to the coordinator plugin.
The coordinator then transmits the hyperparameters and data to the \gls{of} plugin's \emph{pass\_data} endpoint.
In this setup, the minimizer plugin must also store the hyperparameters and data since it must relay this to the \gls{of} plugin's loss calculation endpoint.
Listing \ref{lst:calc_loss_schema_multistep} exhibits the input schema for the \emph{calc\_loss} endpoint, while Listing \ref{lst:minimization_schema_multistep} demonstrates the \emph{minimization} endpoint's schema.
Passing the hyperparameters of a \gls{of} plugin ensures that neither the coordinator nor the minimizer plugin requires in-depth knowledge of the hyperparameters irrespective of the \gls{of} 's internal mechanics.
As a trade-off, there is no need to relay a task's database ID to the coordinator.

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=\emph{CalcLossOrGradInputSchema} Schema for non-multistep-plugin approach., label=lst:calc_loss_schema_multistep]{code/calc_loss_schema_2.py}
\end{minipage}

\noindent\begin{minipage}{\linewidth}
  \lstinputlisting[language=Python, caption=\emph{MinimizerInputSchema} Schema for non-multistep-plugin approach., label=lst:minimization_schema_multistep]{code/minimization_schema_2.py}
\end{minipage}


\chapter{Measurable Results}
\label{chap:results}

This chapter explains the outcomes achieved with the current implementation, emphasizing system performance, plugin interchangeability, and developer usability.

Performance benchmarks are conducted on a MacBook Pro. The system's detailed specifications include:

\begin{itemize}
    \item \textbf{Model:} MacBookPro18,1
    \item \textbf{CPU:} Apple M1 Pro
    \item \textbf{RAM:} 32 GB
    \item \textbf{GPU:} Apple M1 Pro (16 Cores, Metal 3 support)
    \item \textbf{Operating System:} macOS Version 13.4.1
\end{itemize}


\section{Benchmark Results}
\label{subsec:benchmarkingResults}

The most resource-intensive phase of the minimization process is related to the loss function's computation, given its recurrent invocation.
The benchmarks employed the \emph{ridge-loss} plugin as the \gls{of}, configuring \emph{alpha} at 0.5.
The \emph{scipy-minimizer} plugin functioned as the minimizer, with the method set as \emph{L-BFGS-B}.

Table \ref{table:of_calls} displays the frequency of loss evaluations during the minimization process for varying datasets, underscoring the significance of efficient loss function computation.

\begin{table}[h!]
  \centering
  \begin{tabular}{cc}
  \toprule
  \textbf{Number of Data Points} & \textbf{Number of Evaluations} \\
  \midrule
  200 & 264 \\
  400 & 341 \\
  600 & 370 \\
  800 & 430 \\
  1000 & 480 \\
  1200 & 520 \\
  1400 & 570 \\
  \bottomrule
  \end{tabular}
  \caption{Number of times the loss function gets evaluated during minimization across different dataset sizes.}
  \label{table:of_calls}
\end{table}

Figure \ref{fig:of_call_time_version1} shows the time it takes for a single call of the loss function for different dataset sizes for the first plugin-based implementation approach.
The \emph{Calculation Time} captures only the actual time it takes to calculate ridge loss.
The \emph{Database Time} measures the time it takes to retrieve the data from the database.
The \emph{Network Time} measures the time it takes to send the data to the \gls{of} plugin and receive the result.
The total time it takes from the point at which the minimizer plugin calls the calculation endpoint until it receives the result is the sum of the \emph{Calculation Time}, \emph{Database Time}, and \emph{Network Time}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphics/of_call_times_version1.png}
  \caption{Time it takes for a single call to the \emph{calc\_loss} endpoint of the \emph{ridge-loss} plugin for different dataset sizes for the first plugin-based implementation approach.}
  \label{fig:of_call_time_version1}
\end{figure}

Figure \ref{fig:of_call_time_version2} captures the same measurements as \ref{fig:of_call_time_version1} but for the second plugin-based implementation approach.
The diagram does not show the \emph{Database Time} since this version does not make calls to the database.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphics/of_call_times_version2.png}
  \caption{Time it takes for a single call to the \emph{calc\_loss} endpoint of the \emph{ridge-loss} plugin for different dataset sizes for the second plugin-based implementation approach.}
  \label{fig:of_call_time_version2}
\end{figure}

Figure \ref{fig:coparison_of_of} compares the total times it takes to get the loss value across different implementations, including the Jupyter Notebook implementation.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphics/comparison_of_of.png}
  \caption{Comparison of the total time it takes to get the loss value across different implementations, including the Jupyter Notebook implementation.}
  \label{fig:coparison_of_of}
\end{figure}

Figure \ref{fig:time_for_of_calc} measures the same metrics as Figure \ref{fig:coparison_of_of}, but this time it ignores the \emph{Database Time} and the \emph{Network Time} only measuring the actual \emph{Calculation Time}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphics/time_for_of_calc.png}
  \caption{Comparison of the time the actual loss calculation takes across different implementations, including the Jupyter Notebook implementation.}
  \label{fig:time_for_of_calc}
\end{figure}

One can see that the Jupyter Notebook implementation is the fastest as it does not have to deal with network or database latency.
Especially for the first plugin-based implementation approach, the database latency is a bottleneck that can be avoided.

\section{Caching of Data}
\label{subsec:cachingOfData}

To avoid database latency, a caching mechanism is implemented for the first plugin-based implementation approach.
Since the input and target data and the hyperparameters do not change during the minimization process, they can be cached.
That means the data is retrieved from the database during the first call of the \emph{calc\_loss} endpoint and cached for subsequent calls.
For the implementation the \emph{flask-caching} library is used and the cached data is kept in memory.
Figure \ref{fig:of_call_time_version1_cached} shows the average time it takes for a single call of the \emph{calc\_loss} endpoint for different dataset sizes when using caching.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphics/of_call_times_version1_cached.png}
  \caption{Time it takes for a single call to the \emph{calc\_loss} endpoint of the \emph{ridge-loss} plugin for different dataset sizes with caching.}
  \label{fig:of_call_time_version1_cached}
\end{figure}

Figure \ref{fig:coparison_of_of_cached} compares the total times it takes to get the loss across different implementations, including a cached version of the first plugin-based implementation approach.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphics/comparison_of_of_cached.png}
  \caption{Comparison of the total time it takes to get the loss value across different implementations, including a cached version of the first plugin-based implementation approach.}
  \label{fig:coparison_of_of_cached}
\end{figure}

Finally, Figure \ref{fig:time_for_minimization} shows the total time it takes to complete the minimization process for the different implementations.
The measurements for the plugin-based implementations do not include the time it takes a user to input data but instead starts when the coordinator plugin calls the minimization endpoint of the minimizer plugin.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{graphics/time_for_minimization.png}
  \caption{Comparison of the total time it takes to complete the minimization process across different implementations.}
  \label{fig:time_for_minimization}
\end{figure}

\newpage
\section{Interchangeability and Standardization}
\label{sec:interchangeabilityOfPlugins}

This segment evaluates the plugins' adherence to predefined standards and their interchangeability.
The governing standards, as enumerated in Chapters \ref{chap:architecture} and \ref{chap:implementation}, encompass:

\begin{description}
  \item[Metadata] All invocable plugins should furnish interaction endpoints in their metadata.
  \item[Interaction Endpoints] Such plugins must also offer interaction endpoints compliant with respective argument and response schemas.
  \item[Callback Protocols] Both invoking and invoked plugins should uphold the callback mechanisms delineated in Section \ref{sec:implementationOfCallbacksToTheCoordinatorPlugin}.
  \item[Functionality] Each plugin should retain its intended functionality as specified in Section \ref{sec:decomposition}.
\end{description}

Successful compliance ensures plugin interchangeability.
All example plugins in the implementation segment conform to these standards, ensuring mutual compatibility.
Users can combine any minimizer plugin with any \gls{of} without changing any code.
It is important to note, though, that to leverage the gradient calculation provided by the \emph{neural-network} plugin, the \emph{scipy-minimizer-grad} plugin has to be used.

The combination of all minimizer plugins with all \gls{of} plugins is extensively tested manually, and the results have been documented in Table \ref{table:interchangeability} to ensure that the plugins are fully interchangeable.


\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    & \textbf{ridge-loss} & \textbf{hinge-loss} & \textbf{neural-network} \\
    \hline
    \textbf{scipy-minimizer} & \checkmark & \checkmark & \checkmark \\
    \hline
    \textbf{scipy-minimizer-grad} & \checkmark & \checkmark & \checkmark \\
    \hline
  \end{tabular}
  \caption{Compatibility between example implementation of plugins.}
  \label{table:interchangeability}
\end{table}



\section{Developer Usability Assessment}
\label{sec:usabilityForDevelopers}

This section assesses the system's developer-friendliness.
The steps required to implement new minimizer and \gls{of} plugins are documented to underpin the evaluation.
While every plugin's inception is based on an existing template, the first step is to copy the template into a new directory.

\paragraph{Minimizer Plugin Development:}
\label{subsec:implementingAMinimizerPlugin}

For crafting a novel minimizer plugin:

\begin{enumerate}
  \item Replicate the \emph{scipy-minimizer} plugin into a distinct directory.
  \item Update the plugin's name in the \emph{\_\_init\_\_.py} file.
  \item Integrate requisite hyperparameters into the \emph{MinimizerSetupTaskInputSchema} schema.
  \item Implement the minimization algorithm within the \emph{minimize\_task} function.
\end{enumerate}

\paragraph{Objective Function Plugin Development:}
\label{subsec:implementingAnObjectiveFunctionPlugin}

For devising a new \gls{of} plugin:

\begin{enumerate}
  \item Duplicate the \emph{ridge-loss} plugin into a fresh directory.
  \item Modify the plugin's name in the \emph{\_\_init\_\_.py} file.
  \item Incorporate necessary hyperparameters into the \emph{HyperparamterInputSchema} schema.
  \item Implement the input weight calculation within the \emph{of\_pass\_data} endpoint.
  \item Construct the loss function's computation in the \emph{calc\_loss} endpoint.
\end{enumerate}


\chapter{Discussion}
\label{chap:discussion}

This chapter delves into the critical discussions surrounding architectural choices, the model's practical execution, and the consequential outcomes.
It starts by evaluating the plugin-based optimization framework's architectural decisions and how it is implemented.
Subsequently, it analyzes the performance of the plugin-based optimization framework and compares it to the Jupyter Notebook implementation.
Finally, this concept addresses how the results influence the developer experience when it comes to implementing new minimization and \gls{of} plugins.

\section{Architecture and Implementation}
\paragraph{Decomposition:}
\label{sec:decomposition}

Decomposing the minimization process into separate minimizer and \gls{of} plugins aligns perfectly with the plugin-based optimization framework's ethos.
This delineation ensures clarity and specificity and establishes a transparent interface between the two plugins.
The success of libraries like SciPy and how it handles the minimization process \cite{Virtanen2020} reinforces the effectiveness of this decomposition.
Entrusting the coordinator plugin with the responsibility of overseeing the optimization eliminates the need for the minimizer and \gls{of} plugins to manage interactions, enabling developers to concentrate solely on enhancing plugin functionalities.

\paragraph{Endpoint Interactions:}
\label{sec:enpointinteractions}

Interaction endpoints serve as the core of the plugin-based optimization framework, offering a unified mode of interaction across plugins.
This standardization supports developers by delineating available endpoints and their operational functionalities.
By presenting these endpoints even before passing through the \gls{ui} process, developers can bypass the \gls{ui} and directly trigger the requisite endpoint.
Such a setup is invaluable for those seeking automation.
By precisely defining input-output expectations, this approach also ensures seamless plugin interchangeability.

\paragraph{Callback Protocols:}
\label{sec:callackMechanism}

The callback mechanism is an essential facet of the plugin-based optimization framework.
Its design ensures a distinct separation of responsibilities.
The coordinator plugin, devoid of internal insights into the minimizer or \gls{of} plugin, solely relies on available endpoints.
This mechanism prevents the need for database polling, offering real-time updates upon minimization completion.
The distinctive implementation of the callback mechanism for the minimization endpoint further underlines the modularity, as it fully decouples the procedure from the minimizer plugin.
The event-driven model drives adaptability for all kinds of asynchronous processes in \gls{qhana}

\paragraph{Data Passing Endpoint:}
\label{sec:passDataEndpoint}

The rationale behind the \emph{pass\_data} endpoint merits discussion.
While it introduces an additional layer, the trade-offs justify its existence.
Directly passing vast datasets as query parameters to the \gls{ui} endpoint is not feasible.
Transferring data as files to the \gls{ui} endpoint burdens the \gls{of} plugin with data extraction – a responsibility it should not shoulder.
Receiving data as a NumPy array allows the \gls{of} plugin to focus on its primary role: loss value computation.

\paragraph{Comparision of Plugin-Based Implementation Strategies:}
\label{par:differencesBetweenTheTwoPluginBasedImplementationApproaches}

The two plugin-based implementation strategies show the trade-off between the complexity of handling interaction endpoints and the amount of data that has to be passed between plugins.

The first approach, where the \gls{of} plugin is implemented as a multi-step plugin, is more complex since the coordinator plugin has to format the interaction endpoint URL to reflect the database ID.
Nevertheless, it also has the advantage that the minimizer plugin does not have to pass the hyperparameters and data to the \gls{of} plugin since the \gls{of} plugin retrieves them from the database.
In general, that means by passing along a single parameter, the database ID, any volume of data can be loaded and used for context.

The second approach, where the \gls{of} plugin is not implemented as a multi-step plugin, is less complex since the coordinator plugin does not have to handle formatting the interaction endpoint URL.
However, it also has the disadvantage that the minimizer plugin has to pass the hyperparameters and data to the \gls{of} plugin since the \gls{of} plugin cannot retrieve them from the database.

Both approaches have advantages and disadvantages and should show that the plugin-based optimization framework is flexible enough to support both approaches.
As the benchmarking results show, by caching the data, the first approach is arguably the better one of the two.


\paragraph{Plugin Interchangeability and Standardization:}
\label{sec:interchangeabilityAndStandardization}

Standardization is paramount in the plugin-based optimization framework.
The metadata, interaction endpoints, callback protocols, and plugin functionalities are meticulously defined to foster uniformity.
This organized structure creates seamless plugin interchangeability.
The modular nature of this framework not only simplifies the optimization process but also encourages a modular development course, paving the path for universal plugin interactions within \gls{qhana}.

\section{Performance Insights}
\label{sec:performanceAnalysis}

Performance metrics underscore the plugin-based optimization framework's viability when compared against the Jupyter Notebook model.
Absent caching, both plugin methodologies suffer from network and database latencies.
However, Figure \ref{fig:time_for_of_calc} stands out, highlighting that the intrinsic time for loss function computation remains unaffected across methods.

Caching within the first plugin-based model increases performance significantly, as evidenced by Figure \ref{fig:of_call_time_version1_cached}.
However, network latencies persist and cannot be minimized further as data sent and received is already minimal.
Additionally, some database overhead is still present, as the data has to be retrieved from the database once and stored in the cache.
As dataset sizes increase, these latencies diminish in relative significance.
When fortified with caching, the plugin-based approach emerges as a compelling alternative to the Jupyter Notebook implementation.

\section{Developer Experience}
\label{sec:developerUsabilityAssessment}

The plugin-based optimization framework manifests developer-centricity.
Shielded from the difficulties of plugin interactions, developers can emphasize plugin functionalities.
The regimented structure of the interaction endpoints liberates developers from concerns about other plugins.
This contrasts with the Jupyter Notebook implementation, where developers must know the entire optimization process when implementing a new minimizer or \gls{of}.
As the results in \ref{sec:usabilityForDevelopers} show, implementing new minimizer and \gls{of} plugins is straightforward and can be done in a few steps.
What simplifies the process even more is that all plugins of the same type have the same structure, meaning that the first step is only to copy an existing plugin.
By copying an existing plugin, everything concerning the plugin's metadata, interaction endpoints, and callback protocols is already implemented.
The standardized architecture means developers can focus on hyperparameter customization and core calculations during implementation.
With several existing example plugins, developers can leverage these to understand how the core implementations are realized.
The understanding is further enhanced by the detailed documentation of the plugin-based optimization framework that explains the architecture and implementation.


\chapter{Related Work}
\label{chap:relatedWork}

Beisel et al. \cite{Beisel2023} detail the implementation of \glspl{vqa} within a workflow-based system.
They delve into the intricacies of incorporating the quantum aspect of the algorithm into such a system while emphasizing the interaction between its various components less.
In their approach, the conventional computing segment of the algorithm is split into two services: the \textbf{Objective Evaluation Service} and the \textbf{Optimization Service}.
Similar to this thesis, the \textbf{Optimization Service} utilizes SciPy for optimization tasks.

In a subsequent paper \cite{Beisel2023a}, Beisel et al. elaborate on the architecture of the workflow-based system, shedding light on the interplay between its components.
They explain that the system's components are realized as microservices, each offering a \gls{rest}ful API for interaction.
Central to this architecture is a \textbf{Gateway}, which handles the internal communication with the various microservices and presents a unified \gls{rest}ful interface to the outside.

Mayer et al. \cite{Mayer_2003} advocate simplifying the intricacies of expansive software systems through a \emph{plugin-based architecture}.
They argue that the system gains flexibility and becomes more maintainable by minimizing the core system and encapsulating most functionalities as plugins.
They introduce the concept of \textbf{Plugin-in Component Architecture} (PICA), where each plugin must provide a list of interfaces that the core can call and a description of how to use the interfaces.

Wolfinger et al. \cite{Wolfinger2006} present a plugin architecture tailored for the .NET platform, drawing parallels with the Eclipse platform.
The authors emphasize the structured interaction between plugins through the concepts of \textbf{slots} and \textbf{extensions}.
A plugin host defines slots, indicating how they can be extended, while plugin contributors provide extensions to fill these slots.
This structured interaction ensures seamless integration of plugins.
The architecture also leverages .NET features to embed relevant plugin information directly within the application's source code, arguing for improved readability and maintainability.

Birsan et al. \cite{Birsan2005} delve into the {plugin architectures of the Eclipse platform.
Birsan underscores the importance of well-defined \textbf{extension points}, allowing plugins to interact seamlessly.
The work highlights the intricate interplay between plugins, emphasizing the need for structured interfaces for effective interaction.

Thullier et al. \cite{Thullier2021} introduce a \emph{machine learning workbench}, emphasizing a modular approach to machine learning processes for smart homes.
The workbench divides the machine learning workflow into distinct services: the \textbf{Windowing Module}, the \textbf{Feature Extraction Module}, and the \textbf{Machine Learning Module}.
This modular design ensures efficient parallel processing and scalability.
The paper does not aim to split the machine learning service into different components.


\chapter{Conclusion and Outlook}
\label{chap:conclusion}

This thesis embarked on the challenging journey of conceptualizing and realizing the plugin-based optimization framework within the \gls{qhana} environment.
What emerged is an innovative approach to optimization and a pioneering framework that establishes protocols for plugin interactions within \gls{qhana}.
This framework's versatility hints at its potential applicability beyond just optimization tasks.

The example implementations of both the minimizer and the \gls{of} plugins are a testament to the system's functionality and adaptability.
Performance benchmarks highlight the framework's robustness and efficiency, especially with caching.
A particularly commendable aspect is the strict adherence to standardization, which ensures modularity and paves the way for seamless plugin interchangeability.
This design philosophy also emphasizes the developer-centric approach, simplifying plugin development while ensuring versatility.

Looking ahead, the horizons for this work are vast.
The current ecosystem provides a substantial opportunity to diversify by introducing a broader array of minimizer and \gls{of} plugins.
Expanding this repertoire will not only enrich the optimization solutions available but also enhance the robustness and adaptability of the system.

As yet to be fully implemented in this thesis, a new \gls{of} plugin that handles \glspl{vqa} would be a valuable and easily implementable addition.
The energy state of a quantum system is then perceived as the loss value, while an existing minimizer plugin is used to minimize this loss.

Building upon the inherent interchangeability and modularity of the current design, a novel concept draws attention: viewing the \gls{of} or the minimizer as hyperparameters themselves.
In a traditional setting, hyperparameters are parameters set before the learning process begins.
However, in our context, given the modularity, one could envision running parallel optimization processes with different \gls{of} plugins with the same minimizer.
This would be similar to a hyperparameter search, where each \gls{of} is a candidate and aims to find the best fitting \gls{of} for a given dataset or problem.
Similarly, different minimizers can be benchmarked against a fixed \gls{of}.

In conclusion, with its modularity, standardization, and user-friendliness, the plugin-based optimization framework offers many opportunities.
From diversifying its offerings to delving deep into quantum algorithms and even reimagining the very nature of optimization components, the journey ahead is teeming with potential.

\printbibliography

\chapter*{Appendix}
\label{chap:appendix}
\lstinputlisting[language=Python, caption={Source code for generating a sample test dataset for benchmarking with 1000 samples, 47 features, 10 noise and 1 target}., label=lst:generate_test_dataset]{code/ridge_loss_data.py}.

\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Versicherung
\end{document}
